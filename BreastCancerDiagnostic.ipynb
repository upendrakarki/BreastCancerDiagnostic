{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#Import libraries\n",
    "import pandas as pd                 \n",
    "import matplotlib.pyplot as plt      \n",
    "import numpy as np\n",
    "import theano \n",
    "import keras \n",
    "import tensorflow\n",
    "%matplotlib inline\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec \n",
    "#visualization\n",
    "import seaborn as sns \n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers.core import Dropout, Flatten, Activation, Dense\n",
    "from keras.layers.convolutional import Convolution2D, Convolution1D,MaxPooling1D\n",
    "#Import models from scikit learn module:\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import KFold   #For cross validation\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier, export_graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(569, 33)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"data.csv\",header=0)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30000, 33)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.sample(n=30000 , replace=True)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30000"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.drop('id',axis=1,inplace=True)\n",
    "df.drop('Unnamed: 32',axis=1,inplace=True)\n",
    "len(df) #size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>diagnosis</th>\n",
       "      <th>radius_mean</th>\n",
       "      <th>texture_mean</th>\n",
       "      <th>perimeter_mean</th>\n",
       "      <th>area_mean</th>\n",
       "      <th>smoothness_mean</th>\n",
       "      <th>compactness_mean</th>\n",
       "      <th>concavity_mean</th>\n",
       "      <th>concave points_mean</th>\n",
       "      <th>symmetry_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>radius_worst</th>\n",
       "      <th>texture_worst</th>\n",
       "      <th>perimeter_worst</th>\n",
       "      <th>area_worst</th>\n",
       "      <th>smoothness_worst</th>\n",
       "      <th>compactness_worst</th>\n",
       "      <th>concavity_worst</th>\n",
       "      <th>concave points_worst</th>\n",
       "      <th>symmetry_worst</th>\n",
       "      <th>fractal_dimension_worst</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>1</td>\n",
       "      <td>14.48</td>\n",
       "      <td>21.46</td>\n",
       "      <td>94.25</td>\n",
       "      <td>648.2</td>\n",
       "      <td>0.09444</td>\n",
       "      <td>0.09947</td>\n",
       "      <td>0.120400</td>\n",
       "      <td>0.049380</td>\n",
       "      <td>0.2075</td>\n",
       "      <td>...</td>\n",
       "      <td>16.21</td>\n",
       "      <td>29.25</td>\n",
       "      <td>108.40</td>\n",
       "      <td>808.9</td>\n",
       "      <td>0.13060</td>\n",
       "      <td>0.19760</td>\n",
       "      <td>0.33490</td>\n",
       "      <td>0.1225</td>\n",
       "      <td>0.3020</td>\n",
       "      <td>0.06846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>322</th>\n",
       "      <td>0</td>\n",
       "      <td>12.86</td>\n",
       "      <td>13.32</td>\n",
       "      <td>82.82</td>\n",
       "      <td>504.8</td>\n",
       "      <td>0.11340</td>\n",
       "      <td>0.08834</td>\n",
       "      <td>0.038000</td>\n",
       "      <td>0.034000</td>\n",
       "      <td>0.1543</td>\n",
       "      <td>...</td>\n",
       "      <td>14.04</td>\n",
       "      <td>21.08</td>\n",
       "      <td>92.80</td>\n",
       "      <td>599.5</td>\n",
       "      <td>0.15470</td>\n",
       "      <td>0.22310</td>\n",
       "      <td>0.17910</td>\n",
       "      <td>0.1155</td>\n",
       "      <td>0.2382</td>\n",
       "      <td>0.08553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>1</td>\n",
       "      <td>20.26</td>\n",
       "      <td>23.03</td>\n",
       "      <td>132.40</td>\n",
       "      <td>1264.0</td>\n",
       "      <td>0.09078</td>\n",
       "      <td>0.13130</td>\n",
       "      <td>0.146500</td>\n",
       "      <td>0.086830</td>\n",
       "      <td>0.2095</td>\n",
       "      <td>...</td>\n",
       "      <td>24.22</td>\n",
       "      <td>31.59</td>\n",
       "      <td>156.10</td>\n",
       "      <td>1750.0</td>\n",
       "      <td>0.11900</td>\n",
       "      <td>0.35390</td>\n",
       "      <td>0.40980</td>\n",
       "      <td>0.1573</td>\n",
       "      <td>0.3689</td>\n",
       "      <td>0.08368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>308</th>\n",
       "      <td>0</td>\n",
       "      <td>13.50</td>\n",
       "      <td>12.71</td>\n",
       "      <td>85.69</td>\n",
       "      <td>566.2</td>\n",
       "      <td>0.07376</td>\n",
       "      <td>0.03614</td>\n",
       "      <td>0.002758</td>\n",
       "      <td>0.004419</td>\n",
       "      <td>0.1365</td>\n",
       "      <td>...</td>\n",
       "      <td>14.97</td>\n",
       "      <td>16.94</td>\n",
       "      <td>95.48</td>\n",
       "      <td>698.7</td>\n",
       "      <td>0.09023</td>\n",
       "      <td>0.05836</td>\n",
       "      <td>0.01379</td>\n",
       "      <td>0.0221</td>\n",
       "      <td>0.2267</td>\n",
       "      <td>0.06192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>330</th>\n",
       "      <td>1</td>\n",
       "      <td>16.03</td>\n",
       "      <td>15.51</td>\n",
       "      <td>105.80</td>\n",
       "      <td>793.2</td>\n",
       "      <td>0.09491</td>\n",
       "      <td>0.13710</td>\n",
       "      <td>0.120400</td>\n",
       "      <td>0.070410</td>\n",
       "      <td>0.1782</td>\n",
       "      <td>...</td>\n",
       "      <td>18.76</td>\n",
       "      <td>21.98</td>\n",
       "      <td>124.30</td>\n",
       "      <td>1070.0</td>\n",
       "      <td>0.14350</td>\n",
       "      <td>0.44780</td>\n",
       "      <td>0.49560</td>\n",
       "      <td>0.1981</td>\n",
       "      <td>0.3019</td>\n",
       "      <td>0.09124</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     diagnosis  radius_mean  texture_mean  perimeter_mean  area_mean  \\\n",
       "86           1        14.48         21.46           94.25      648.2   \n",
       "322          0        12.86         13.32           82.82      504.8   \n",
       "95           1        20.26         23.03          132.40     1264.0   \n",
       "308          0        13.50         12.71           85.69      566.2   \n",
       "330          1        16.03         15.51          105.80      793.2   \n",
       "\n",
       "     smoothness_mean  compactness_mean  concavity_mean  concave points_mean  \\\n",
       "86           0.09444           0.09947        0.120400             0.049380   \n",
       "322          0.11340           0.08834        0.038000             0.034000   \n",
       "95           0.09078           0.13130        0.146500             0.086830   \n",
       "308          0.07376           0.03614        0.002758             0.004419   \n",
       "330          0.09491           0.13710        0.120400             0.070410   \n",
       "\n",
       "     symmetry_mean  ...  radius_worst  texture_worst  perimeter_worst  \\\n",
       "86          0.2075  ...         16.21          29.25           108.40   \n",
       "322         0.1543  ...         14.04          21.08            92.80   \n",
       "95          0.2095  ...         24.22          31.59           156.10   \n",
       "308         0.1365  ...         14.97          16.94            95.48   \n",
       "330         0.1782  ...         18.76          21.98           124.30   \n",
       "\n",
       "     area_worst  smoothness_worst  compactness_worst  concavity_worst  \\\n",
       "86        808.9           0.13060            0.19760          0.33490   \n",
       "322       599.5           0.15470            0.22310          0.17910   \n",
       "95       1750.0           0.11900            0.35390          0.40980   \n",
       "308       698.7           0.09023            0.05836          0.01379   \n",
       "330      1070.0           0.14350            0.44780          0.49560   \n",
       "\n",
       "     concave points_worst  symmetry_worst  fractal_dimension_worst  \n",
       "86                 0.1225          0.3020                  0.06846  \n",
       "322                0.1155          0.2382                  0.08553  \n",
       "95                 0.1573          0.3689                  0.08368  \n",
       "308                0.0221          0.2267                  0.06192  \n",
       "330                0.1981          0.3019                  0.09124  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.diagnosis.unique()\n",
    "df['diagnosis'] = df['diagnosis'].map({'M':1,'B':0})\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30000\n",
      "Number of Malignant cases:  11112 (37.04%)\n",
      "Number of Benign cases: 18888 (62.96%)\n"
     ]
    }
   ],
   "source": [
    "num_obs = len(df)\n",
    "print (num_obs)\n",
    "num_true = len(df.loc[df['diagnosis'] == 1])\n",
    "num_false = len(df.loc[df['diagnosis'] == 0])\n",
    "print(\"Number of Malignant cases:  {0} ({1:2.2f}%)\".format(num_true, (float (num_true)/num_obs) * 100))\n",
    "print(\"Number of Benign cases: {0} ({1:2.2f}%)\".format(num_false, (float(num_false)/num_obs) * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "feature_col_names = ['radius_mean', 'texture_mean', 'perimeter_mean',\n",
    "       'area_mean', 'smoothness_mean', 'compactness_mean', 'concavity_mean',\n",
    "       'concave points_mean', 'symmetry_mean', 'fractal_dimension_mean',\n",
    "       'radius_se', 'texture_se', 'perimeter_se', 'area_se', 'smoothness_se',\n",
    "       'compactness_se', 'concavity_se', 'concave points_se', 'symmetry_se',\n",
    "       'fractal_dimension_se', 'radius_worst', 'texture_worst',\n",
    "       'perimeter_worst', 'area_worst', 'smoothness_worst',\n",
    "       'compactness_worst', 'concavity_worst', 'concave points_worst',\n",
    "       'symmetry_worst', 'fractal_dimension_worst']\n",
    "predicted_class_names = ['diagnosis']\n",
    "X = df[feature_col_names].values     \n",
    "y = df[predicted_class_names].values \n",
    "split_test_size = 0.30 #That is use 30% data as test data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=split_test_size, random_state=42) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.9454444444444444\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Upendra\\AppData\\Local\\conda\\conda\\envs\\DeepLearningEnv\\lib\\site-packages\\sklearn\\utils\\validation.py:761: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Upendra\\AppData\\Local\\conda\\conda\\envs\\DeepLearningEnv\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "#Logistic Regression\n",
    "#from sklearn.linear_model import LogisticRegression\n",
    "clf = LogisticRegression(random_state=0, solver='lbfgs',multi_class='ovr').fit(X_train, y_train)\n",
    "y_pred=clf.predict(X_test)\n",
    "\n",
    "clf.predict_proba(X_test) \n",
    "#print(clf.predict_proba(X_test) )\n",
    "\n",
    "print(\"Score:\",clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Upendra\\AppData\\Local\\conda\\conda\\envs\\DeepLearningEnv\\lib\\site-packages\\ipykernel_launcher.py:2: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "clf=RandomForestClassifier(n_estimators=100,random_state=84)\n",
    "clf.fit(X_train,y_train)\n",
    "y_pred=clf.predict(X_test)\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\n",
    "#print(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Neural Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Upendra\\AppData\\Local\\conda\\conda\\envs\\DeepLearningEnv\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Upendra\\AppData\\Local\\conda\\conda\\envs\\DeepLearningEnv\\lib\\site-packages\\ipykernel_launcher.py:2: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(512, input_dim=30, activation=\"sigmoid\", kernel_initializer=\"uniform\")`\n",
      "  \n",
      "C:\\Users\\Upendra\\AppData\\Local\\conda\\conda\\envs\\DeepLearningEnv\\lib\\site-packages\\ipykernel_launcher.py:3: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(1, activation=\"relu\", kernel_initializer=\"uniform\")`\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(512, input_dim=30, init='uniform', activation='sigmoid'))\n",
    "model.add(Dense(1, init='uniform', activation='relu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='mean_squared_error', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Upendra\\AppData\\Local\\conda\\conda\\envs\\DeepLearningEnv\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 21000 samples, validate on 9000 samples\n",
      "Epoch 1/600\n",
      " - 0s - loss: 0.0758 - acc: 0.8946 - val_loss: 0.0542 - val_acc: 0.9346\n",
      "Epoch 2/600\n",
      " - 0s - loss: 0.0538 - acc: 0.9304 - val_loss: 0.0483 - val_acc: 0.9384\n",
      "Epoch 3/600\n",
      " - 0s - loss: 0.0494 - acc: 0.9364 - val_loss: 0.0588 - val_acc: 0.9349\n",
      "Epoch 4/600\n",
      " - 0s - loss: 0.0498 - acc: 0.9344 - val_loss: 0.0436 - val_acc: 0.9391\n",
      "Epoch 5/600\n",
      " - 0s - loss: 0.0450 - acc: 0.9400 - val_loss: 0.0629 - val_acc: 0.9248\n",
      "Epoch 6/600\n",
      " - 0s - loss: 0.0438 - acc: 0.9417 - val_loss: 0.0701 - val_acc: 0.9138\n",
      "Epoch 7/600\n",
      " - 0s - loss: 0.0474 - acc: 0.9397 - val_loss: 0.0362 - val_acc: 0.9553\n",
      "Epoch 8/600\n",
      " - 0s - loss: 0.0375 - acc: 0.9503 - val_loss: 0.0631 - val_acc: 0.9250\n",
      "Epoch 9/600\n",
      " - 0s - loss: 0.0417 - acc: 0.9444 - val_loss: 0.0678 - val_acc: 0.9137\n",
      "Epoch 10/600\n",
      " - 0s - loss: 0.0383 - acc: 0.9491 - val_loss: 0.0336 - val_acc: 0.9610\n",
      "Epoch 11/600\n",
      " - 0s - loss: 0.0337 - acc: 0.9559 - val_loss: 0.0371 - val_acc: 0.9566\n",
      "Epoch 12/600\n",
      " - 0s - loss: 0.0314 - acc: 0.9589 - val_loss: 0.0320 - val_acc: 0.9579\n",
      "Epoch 13/600\n",
      " - 0s - loss: 0.0315 - acc: 0.9584 - val_loss: 0.1003 - val_acc: 0.8756\n",
      "Epoch 14/600\n",
      " - 0s - loss: 0.0418 - acc: 0.9457 - val_loss: 0.0546 - val_acc: 0.9246\n",
      "Epoch 15/600\n",
      " - 0s - loss: 0.0336 - acc: 0.9541 - val_loss: 0.0663 - val_acc: 0.9118\n",
      "Epoch 16/600\n",
      " - 0s - loss: 0.0328 - acc: 0.9564 - val_loss: 0.0377 - val_acc: 0.9608\n",
      "Epoch 17/600\n",
      " - 0s - loss: 0.0296 - acc: 0.9629 - val_loss: 0.0257 - val_acc: 0.9649\n",
      "Epoch 18/600\n",
      " - 0s - loss: 0.0261 - acc: 0.9639 - val_loss: 0.0262 - val_acc: 0.9647\n",
      "Epoch 19/600\n",
      " - 0s - loss: 0.0251 - acc: 0.9663 - val_loss: 0.0314 - val_acc: 0.9568\n",
      "Epoch 20/600\n",
      " - 0s - loss: 0.0263 - acc: 0.9671 - val_loss: 0.0255 - val_acc: 0.9703\n",
      "Epoch 21/600\n",
      " - 0s - loss: 0.0252 - acc: 0.9679 - val_loss: 0.0296 - val_acc: 0.9641\n",
      "Epoch 22/600\n",
      " - 0s - loss: 0.0245 - acc: 0.9700 - val_loss: 0.1765 - val_acc: 0.7782\n",
      "Epoch 23/600\n",
      " - 0s - loss: 0.0488 - acc: 0.9360 - val_loss: 0.0503 - val_acc: 0.9370\n",
      "Epoch 24/600\n",
      " - 0s - loss: 0.0281 - acc: 0.9630 - val_loss: 0.0294 - val_acc: 0.9611\n",
      "Epoch 25/600\n",
      " - 0s - loss: 0.0250 - acc: 0.9659 - val_loss: 0.0265 - val_acc: 0.9702\n",
      "Epoch 26/600\n",
      " - 0s - loss: 0.0268 - acc: 0.9639 - val_loss: 0.0377 - val_acc: 0.9532\n",
      "Epoch 27/600\n",
      " - 0s - loss: 0.0279 - acc: 0.9629 - val_loss: 0.0241 - val_acc: 0.9664\n",
      "Epoch 28/600\n",
      " - 0s - loss: 0.0241 - acc: 0.9676 - val_loss: 0.0590 - val_acc: 0.9292\n",
      "Epoch 29/600\n",
      " - 0s - loss: 0.0300 - acc: 0.9628 - val_loss: 0.0320 - val_acc: 0.9633\n",
      "Epoch 30/600\n",
      " - 0s - loss: 0.0268 - acc: 0.9666 - val_loss: 0.0796 - val_acc: 0.8983\n",
      "Epoch 31/600\n",
      " - 0s - loss: 0.0434 - acc: 0.9449 - val_loss: 0.0274 - val_acc: 0.9654\n",
      "Epoch 32/600\n",
      " - 0s - loss: 0.0284 - acc: 0.9637 - val_loss: 0.0300 - val_acc: 0.9597\n",
      "Epoch 33/600\n",
      " - 0s - loss: 0.0240 - acc: 0.9666 - val_loss: 0.0243 - val_acc: 0.9749\n",
      "Epoch 34/600\n",
      " - 0s - loss: 0.0242 - acc: 0.9698 - val_loss: 0.0237 - val_acc: 0.9683\n",
      "Epoch 35/600\n",
      " - 0s - loss: 0.0224 - acc: 0.9715 - val_loss: 0.0294 - val_acc: 0.9664\n",
      "Epoch 36/600\n",
      " - 0s - loss: 0.0237 - acc: 0.9695 - val_loss: 0.0490 - val_acc: 0.9456\n",
      "Epoch 37/600\n",
      " - 0s - loss: 0.0273 - acc: 0.9659 - val_loss: 0.0830 - val_acc: 0.8887\n",
      "Epoch 38/600\n",
      " - 0s - loss: 0.0306 - acc: 0.9605 - val_loss: 0.0252 - val_acc: 0.9664\n",
      "Epoch 39/600\n",
      " - 0s - loss: 0.0225 - acc: 0.9701 - val_loss: 0.0212 - val_acc: 0.9683\n",
      "Epoch 40/600\n",
      " - 0s - loss: 0.0227 - acc: 0.9703 - val_loss: 0.0300 - val_acc: 0.9633\n",
      "Epoch 41/600\n",
      " - 0s - loss: 0.0238 - acc: 0.9708 - val_loss: 0.0208 - val_acc: 0.9698\n",
      "Epoch 42/600\n",
      " - 0s - loss: 0.0203 - acc: 0.9745 - val_loss: 0.0209 - val_acc: 0.9751\n",
      "Epoch 43/600\n",
      " - 0s - loss: 0.0204 - acc: 0.9731 - val_loss: 0.0391 - val_acc: 0.9483\n",
      "Epoch 44/600\n",
      " - 0s - loss: 0.0270 - acc: 0.9664 - val_loss: 0.0240 - val_acc: 0.9698\n",
      "Epoch 45/600\n",
      " - 0s - loss: 0.0210 - acc: 0.9727 - val_loss: 0.0199 - val_acc: 0.9733\n",
      "Epoch 46/600\n",
      " - 0s - loss: 0.0211 - acc: 0.9734 - val_loss: 0.0205 - val_acc: 0.9762\n",
      "Epoch 47/600\n",
      " - 0s - loss: 0.0195 - acc: 0.9759 - val_loss: 0.0588 - val_acc: 0.9286\n",
      "Epoch 48/600\n",
      " - 0s - loss: 0.0296 - acc: 0.9617 - val_loss: 0.0215 - val_acc: 0.9742\n",
      "Epoch 49/600\n",
      " - 0s - loss: 0.0196 - acc: 0.9757 - val_loss: 0.0200 - val_acc: 0.9762\n",
      "Epoch 50/600\n",
      " - 0s - loss: 0.0194 - acc: 0.9760 - val_loss: 0.0227 - val_acc: 0.9736\n",
      "Epoch 51/600\n",
      " - 0s - loss: 0.0206 - acc: 0.9746 - val_loss: 0.0591 - val_acc: 0.9278\n",
      "Epoch 52/600\n",
      " - 0s - loss: 0.0247 - acc: 0.9706 - val_loss: 0.0209 - val_acc: 0.9706\n",
      "Epoch 53/600\n",
      " - 0s - loss: 0.0189 - acc: 0.9762 - val_loss: 0.0212 - val_acc: 0.9756\n",
      "Epoch 54/600\n",
      " - 0s - loss: 0.0212 - acc: 0.9743 - val_loss: 0.0199 - val_acc: 0.9743\n",
      "Epoch 55/600\n",
      " - 0s - loss: 0.0190 - acc: 0.9761 - val_loss: 0.0194 - val_acc: 0.9744\n",
      "Epoch 56/600\n",
      " - 0s - loss: 0.0186 - acc: 0.9775 - val_loss: 0.0195 - val_acc: 0.9782\n",
      "Epoch 57/600\n",
      " - 0s - loss: 0.0189 - acc: 0.9777 - val_loss: 0.0450 - val_acc: 0.9437\n",
      "Epoch 58/600\n",
      " - 0s - loss: 0.0226 - acc: 0.9735 - val_loss: 0.0262 - val_acc: 0.9658\n",
      "Epoch 59/600\n",
      " - 0s - loss: 0.0200 - acc: 0.9752 - val_loss: 0.0796 - val_acc: 0.9004\n",
      "Epoch 60/600\n",
      " - 0s - loss: 0.0316 - acc: 0.9616 - val_loss: 0.0275 - val_acc: 0.9696\n",
      "Epoch 61/600\n",
      " - 0s - loss: 0.0234 - acc: 0.9700 - val_loss: 0.0227 - val_acc: 0.9726\n",
      "Epoch 62/600\n",
      " - 0s - loss: 0.0190 - acc: 0.9772 - val_loss: 0.0202 - val_acc: 0.9747\n",
      "Epoch 63/600\n",
      " - 0s - loss: 0.0179 - acc: 0.9792 - val_loss: 0.0208 - val_acc: 0.9740\n",
      "Epoch 64/600\n",
      " - 0s - loss: 0.0198 - acc: 0.9766 - val_loss: 0.0756 - val_acc: 0.9086\n",
      "Epoch 65/600\n",
      " - 0s - loss: 0.0262 - acc: 0.9681 - val_loss: 0.0550 - val_acc: 0.9340\n",
      "Epoch 66/600\n",
      " - 0s - loss: 0.0234 - acc: 0.9701 - val_loss: 0.0194 - val_acc: 0.9749\n",
      "Epoch 67/600\n",
      " - 0s - loss: 0.0175 - acc: 0.9784 - val_loss: 0.0242 - val_acc: 0.9729\n",
      "Epoch 68/600\n",
      " - 0s - loss: 0.0185 - acc: 0.9785 - val_loss: 0.0182 - val_acc: 0.9782\n",
      "Epoch 69/600\n",
      " - 0s - loss: 0.0180 - acc: 0.9781 - val_loss: 0.0276 - val_acc: 0.9688\n",
      "Epoch 70/600\n",
      " - 0s - loss: 0.0184 - acc: 0.9773 - val_loss: 0.0200 - val_acc: 0.9754\n",
      "Epoch 71/600\n",
      " - 0s - loss: 0.0176 - acc: 0.9783 - val_loss: 0.0181 - val_acc: 0.9763\n",
      "Epoch 72/600\n",
      " - 0s - loss: 0.0175 - acc: 0.9794 - val_loss: 0.0186 - val_acc: 0.9739\n",
      "Epoch 73/600\n",
      " - 0s - loss: 0.0185 - acc: 0.9775 - val_loss: 0.0190 - val_acc: 0.9772\n",
      "Epoch 74/600\n",
      " - 0s - loss: 0.0176 - acc: 0.9785 - val_loss: 0.0179 - val_acc: 0.9782\n",
      "Epoch 75/600\n",
      " - 0s - loss: 0.0178 - acc: 0.9785 - val_loss: 0.0180 - val_acc: 0.9782\n",
      "Epoch 76/600\n",
      " - 0s - loss: 0.0178 - acc: 0.9787 - val_loss: 0.0228 - val_acc: 0.9717\n",
      "Epoch 77/600\n",
      " - 0s - loss: 0.0180 - acc: 0.9796 - val_loss: 0.0182 - val_acc: 0.9763\n",
      "Epoch 78/600\n",
      " - 0s - loss: 0.0175 - acc: 0.9805 - val_loss: 0.0173 - val_acc: 0.9776\n",
      "Epoch 79/600\n",
      " - 0s - loss: 0.0168 - acc: 0.9808 - val_loss: 0.0179 - val_acc: 0.9813\n",
      "Epoch 80/600\n",
      " - 0s - loss: 0.0167 - acc: 0.9814 - val_loss: 0.0444 - val_acc: 0.9458\n",
      "Epoch 81/600\n",
      " - 0s - loss: 0.0247 - acc: 0.9690 - val_loss: 0.0192 - val_acc: 0.9774\n",
      "Epoch 82/600\n",
      " - 0s - loss: 0.0170 - acc: 0.9794 - val_loss: 0.0231 - val_acc: 0.9724\n",
      "Epoch 83/600\n",
      " - 0s - loss: 0.0177 - acc: 0.9806 - val_loss: 0.0310 - val_acc: 0.9652\n",
      "Epoch 84/600\n",
      " - 0s - loss: 0.0186 - acc: 0.9788 - val_loss: 0.0267 - val_acc: 0.9647\n",
      "Epoch 85/600\n",
      " - 0s - loss: 0.0179 - acc: 0.9781 - val_loss: 0.0165 - val_acc: 0.9794\n",
      "Epoch 86/600\n",
      " - 0s - loss: 0.0190 - acc: 0.9775 - val_loss: 0.0742 - val_acc: 0.9099\n",
      "Epoch 87/600\n",
      " - 0s - loss: 0.0293 - acc: 0.9640 - val_loss: 0.0298 - val_acc: 0.9653\n",
      "Epoch 88/600\n",
      " - 0s - loss: 0.0185 - acc: 0.9783 - val_loss: 0.0207 - val_acc: 0.9743\n",
      "Epoch 89/600\n",
      " - 0s - loss: 0.0184 - acc: 0.9790 - val_loss: 0.0237 - val_acc: 0.9734\n",
      "Epoch 90/600\n",
      " - 0s - loss: 0.0188 - acc: 0.9782 - val_loss: 0.1237 - val_acc: 0.8423\n",
      "Epoch 91/600\n",
      " - 0s - loss: 0.0298 - acc: 0.9633 - val_loss: 0.0246 - val_acc: 0.9657\n",
      "Epoch 92/600\n",
      " - 0s - loss: 0.0202 - acc: 0.9753 - val_loss: 0.0242 - val_acc: 0.9677\n",
      "Epoch 93/600\n",
      " - 0s - loss: 0.0197 - acc: 0.9769 - val_loss: 0.0763 - val_acc: 0.8996\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 94/600\n",
      " - 0s - loss: 0.0256 - acc: 0.9671 - val_loss: 0.0225 - val_acc: 0.9740\n",
      "Epoch 95/600\n",
      " - 0s - loss: 0.0195 - acc: 0.9754 - val_loss: 0.0322 - val_acc: 0.9550\n",
      "Epoch 96/600\n",
      " - 0s - loss: 0.0197 - acc: 0.9770 - val_loss: 0.0182 - val_acc: 0.9826\n",
      "Epoch 97/600\n",
      " - 0s - loss: 0.0174 - acc: 0.9802 - val_loss: 0.0179 - val_acc: 0.9763\n",
      "Epoch 98/600\n",
      " - 0s - loss: 0.0174 - acc: 0.9802 - val_loss: 0.0505 - val_acc: 0.9407\n",
      "Epoch 99/600\n",
      " - 0s - loss: 0.0254 - acc: 0.9690 - val_loss: 0.0190 - val_acc: 0.9801\n",
      "Epoch 100/600\n",
      " - 0s - loss: 0.0184 - acc: 0.9788 - val_loss: 0.0262 - val_acc: 0.9650\n",
      "Epoch 101/600\n",
      " - 0s - loss: 0.0211 - acc: 0.9738 - val_loss: 0.0207 - val_acc: 0.9758\n",
      "Epoch 102/600\n",
      " - 0s - loss: 0.0173 - acc: 0.9798 - val_loss: 0.0471 - val_acc: 0.9364\n",
      "Epoch 103/600\n",
      " - 0s - loss: 0.0247 - acc: 0.9692 - val_loss: 0.0715 - val_acc: 0.9180\n",
      "Epoch 104/600\n",
      " - 0s - loss: 0.0268 - acc: 0.9673 - val_loss: 0.0198 - val_acc: 0.9757\n",
      "Epoch 105/600\n",
      " - 0s - loss: 0.0170 - acc: 0.9802 - val_loss: 0.0175 - val_acc: 0.9782\n",
      "Epoch 106/600\n",
      " - 0s - loss: 0.0166 - acc: 0.9818 - val_loss: 0.0370 - val_acc: 0.9507\n",
      "Epoch 107/600\n",
      " - 0s - loss: 0.0206 - acc: 0.9761 - val_loss: 0.0176 - val_acc: 0.9821\n",
      "Epoch 108/600\n",
      " - 0s - loss: 0.0176 - acc: 0.9797 - val_loss: 0.0177 - val_acc: 0.9802\n",
      "Epoch 109/600\n",
      " - 0s - loss: 0.0166 - acc: 0.9821 - val_loss: 0.0188 - val_acc: 0.9768\n",
      "Epoch 110/600\n",
      " - 0s - loss: 0.0164 - acc: 0.9819 - val_loss: 0.0287 - val_acc: 0.9613\n",
      "Epoch 111/600\n",
      " - 0s - loss: 0.0177 - acc: 0.9804 - val_loss: 0.0263 - val_acc: 0.9663\n",
      "Epoch 112/600\n",
      " - 0s - loss: 0.0184 - acc: 0.9786 - val_loss: 0.0166 - val_acc: 0.9802\n",
      "Epoch 113/600\n",
      " - 0s - loss: 0.0164 - acc: 0.9807 - val_loss: 0.0165 - val_acc: 0.9821\n",
      "Epoch 114/600\n",
      " - 0s - loss: 0.0167 - acc: 0.9805 - val_loss: 0.0202 - val_acc: 0.9742\n",
      "Epoch 115/600\n",
      " - 0s - loss: 0.0173 - acc: 0.9800 - val_loss: 0.0194 - val_acc: 0.9782\n",
      "Epoch 116/600\n",
      " - 0s - loss: 0.0166 - acc: 0.9818 - val_loss: 0.0168 - val_acc: 0.9836\n",
      "Epoch 117/600\n",
      " - 0s - loss: 0.0164 - acc: 0.9823 - val_loss: 0.0202 - val_acc: 0.9742\n",
      "Epoch 118/600\n",
      " - 0s - loss: 0.0173 - acc: 0.9807 - val_loss: 0.0175 - val_acc: 0.9796\n",
      "Epoch 119/600\n",
      " - 0s - loss: 0.0158 - acc: 0.9830 - val_loss: 0.0187 - val_acc: 0.9809\n",
      "Epoch 120/600\n",
      " - 0s - loss: 0.0170 - acc: 0.9816 - val_loss: 0.0165 - val_acc: 0.9802\n",
      "Epoch 121/600\n",
      " - 0s - loss: 0.0156 - acc: 0.9830 - val_loss: 0.0254 - val_acc: 0.9672\n",
      "Epoch 122/600\n",
      " - 0s - loss: 0.0200 - acc: 0.9765 - val_loss: 0.0436 - val_acc: 0.9508\n",
      "Epoch 123/600\n",
      " - 0s - loss: 0.0212 - acc: 0.9752 - val_loss: 0.0484 - val_acc: 0.9489\n",
      "Epoch 124/600\n",
      " - 0s - loss: 0.0268 - acc: 0.9671 - val_loss: 0.0855 - val_acc: 0.8841\n",
      "Epoch 125/600\n",
      " - 0s - loss: 0.0325 - acc: 0.9614 - val_loss: 0.0361 - val_acc: 0.9557\n",
      "Epoch 126/600\n",
      " - 0s - loss: 0.0207 - acc: 0.9771 - val_loss: 0.0195 - val_acc: 0.9831\n",
      "Epoch 127/600\n",
      " - 0s - loss: 0.0176 - acc: 0.9819 - val_loss: 0.0180 - val_acc: 0.9802\n",
      "Epoch 128/600\n",
      " - 0s - loss: 0.0176 - acc: 0.9809 - val_loss: 0.0348 - val_acc: 0.9596\n",
      "Epoch 129/600\n",
      " - 0s - loss: 0.0204 - acc: 0.9773 - val_loss: 0.0205 - val_acc: 0.9761\n",
      "Epoch 130/600\n",
      " - 0s - loss: 0.0176 - acc: 0.9803 - val_loss: 0.0211 - val_acc: 0.9716\n",
      "Epoch 131/600\n",
      " - 0s - loss: 0.0175 - acc: 0.9815 - val_loss: 0.0183 - val_acc: 0.9791\n",
      "Epoch 132/600\n",
      " - 0s - loss: 0.0174 - acc: 0.9817 - val_loss: 0.0218 - val_acc: 0.9752\n",
      "Epoch 133/600\n",
      " - 0s - loss: 0.0167 - acc: 0.9824 - val_loss: 0.0189 - val_acc: 0.9788\n",
      "Epoch 134/600\n",
      " - 0s - loss: 0.0166 - acc: 0.9829 - val_loss: 0.0299 - val_acc: 0.9596\n",
      "Epoch 135/600\n",
      " - 0s - loss: 0.0184 - acc: 0.9792 - val_loss: 0.0182 - val_acc: 0.9806\n",
      "Epoch 136/600\n",
      " - 0s - loss: 0.0161 - acc: 0.9836 - val_loss: 0.0254 - val_acc: 0.9659\n",
      "Epoch 137/600\n",
      " - 0s - loss: 0.0192 - acc: 0.9789 - val_loss: 0.0196 - val_acc: 0.9782\n",
      "Epoch 138/600\n",
      " - 0s - loss: 0.0175 - acc: 0.9801 - val_loss: 0.0176 - val_acc: 0.9806\n",
      "Epoch 139/600\n",
      " - 0s - loss: 0.0167 - acc: 0.9819 - val_loss: 0.0216 - val_acc: 0.9722\n",
      "Epoch 140/600\n",
      " - 0s - loss: 0.0181 - acc: 0.9790 - val_loss: 0.0329 - val_acc: 0.9590\n",
      "Epoch 141/600\n",
      " - 0s - loss: 0.0210 - acc: 0.9769 - val_loss: 0.0174 - val_acc: 0.9802\n",
      "Epoch 142/600\n",
      " - 0s - loss: 0.0153 - acc: 0.9859 - val_loss: 0.0166 - val_acc: 0.9802\n",
      "Epoch 143/600\n",
      " - 0s - loss: 0.0171 - acc: 0.9819 - val_loss: 0.0425 - val_acc: 0.9541\n",
      "Epoch 144/600\n",
      " - 0s - loss: 0.0258 - acc: 0.9692 - val_loss: 0.0177 - val_acc: 0.9802\n",
      "Epoch 145/600\n",
      " - 0s - loss: 0.0154 - acc: 0.9843 - val_loss: 0.0167 - val_acc: 0.9828\n",
      "Epoch 146/600\n",
      " - 0s - loss: 0.0178 - acc: 0.9794 - val_loss: 0.0196 - val_acc: 0.9771\n",
      "Epoch 147/600\n",
      " - 0s - loss: 0.0163 - acc: 0.9831 - val_loss: 0.0177 - val_acc: 0.9823\n",
      "Epoch 148/600\n",
      " - 0s - loss: 0.0167 - acc: 0.9808 - val_loss: 0.0159 - val_acc: 0.9844\n",
      "Epoch 149/600\n",
      " - 0s - loss: 0.0151 - acc: 0.9845 - val_loss: 0.0186 - val_acc: 0.9788\n",
      "Epoch 150/600\n",
      " - 0s - loss: 0.0172 - acc: 0.9806 - val_loss: 0.0159 - val_acc: 0.9840\n",
      "Epoch 151/600\n",
      " - 0s - loss: 0.0151 - acc: 0.9855 - val_loss: 0.0167 - val_acc: 0.9854\n",
      "Epoch 152/600\n",
      " - 0s - loss: 0.0153 - acc: 0.9852 - val_loss: 0.0172 - val_acc: 0.9802\n",
      "Epoch 153/600\n",
      " - 0s - loss: 0.0150 - acc: 0.9853 - val_loss: 0.0180 - val_acc: 0.9802\n",
      "Epoch 154/600\n",
      " - 0s - loss: 0.0150 - acc: 0.9852 - val_loss: 0.0159 - val_acc: 0.9821\n",
      "Epoch 155/600\n",
      " - 0s - loss: 0.0145 - acc: 0.9850 - val_loss: 0.0174 - val_acc: 0.9782\n",
      "Epoch 156/600\n",
      " - 0s - loss: 0.0158 - acc: 0.9838 - val_loss: 0.0229 - val_acc: 0.9751\n",
      "Epoch 157/600\n",
      " - 0s - loss: 0.0242 - acc: 0.9699 - val_loss: 0.0250 - val_acc: 0.9728\n",
      "Epoch 158/600\n",
      " - 0s - loss: 0.0164 - acc: 0.9817 - val_loss: 0.0187 - val_acc: 0.9788\n",
      "Epoch 159/600\n",
      " - 0s - loss: 0.0152 - acc: 0.9830 - val_loss: 0.0165 - val_acc: 0.9843\n",
      "Epoch 160/600\n",
      " - 0s - loss: 0.0147 - acc: 0.9853 - val_loss: 0.0153 - val_acc: 0.9858\n",
      "Epoch 161/600\n",
      " - 0s - loss: 0.0140 - acc: 0.9858 - val_loss: 0.0409 - val_acc: 0.9408\n",
      "Epoch 162/600\n",
      " - 0s - loss: 0.0220 - acc: 0.9730 - val_loss: 0.0171 - val_acc: 0.9810\n",
      "Epoch 163/600\n",
      " - 0s - loss: 0.0165 - acc: 0.9826 - val_loss: 0.0220 - val_acc: 0.9696\n",
      "Epoch 164/600\n",
      " - 0s - loss: 0.0175 - acc: 0.9806 - val_loss: 0.0177 - val_acc: 0.9828\n",
      "Epoch 165/600\n",
      " - 0s - loss: 0.0151 - acc: 0.9860 - val_loss: 0.0165 - val_acc: 0.9863\n",
      "Epoch 166/600\n",
      " - 0s - loss: 0.0150 - acc: 0.9851 - val_loss: 0.0231 - val_acc: 0.9731\n",
      "Epoch 167/600\n",
      " - 0s - loss: 0.0171 - acc: 0.9817 - val_loss: 0.0210 - val_acc: 0.9741\n",
      "Epoch 168/600\n",
      " - 0s - loss: 0.0159 - acc: 0.9846 - val_loss: 0.0164 - val_acc: 0.9821\n",
      "Epoch 169/600\n",
      " - 0s - loss: 0.0147 - acc: 0.9860 - val_loss: 0.0169 - val_acc: 0.9788\n",
      "Epoch 170/600\n",
      " - 0s - loss: 0.0158 - acc: 0.9830 - val_loss: 0.0291 - val_acc: 0.9613\n",
      "Epoch 171/600\n",
      " - 0s - loss: 0.0200 - acc: 0.9769 - val_loss: 0.0158 - val_acc: 0.9877\n",
      "Epoch 172/600\n",
      " - 0s - loss: 0.0145 - acc: 0.9857 - val_loss: 0.0196 - val_acc: 0.9747\n",
      "Epoch 173/600\n",
      " - 0s - loss: 0.0161 - acc: 0.9830 - val_loss: 0.0181 - val_acc: 0.9788\n",
      "Epoch 174/600\n",
      " - 0s - loss: 0.0157 - acc: 0.9837 - val_loss: 0.0187 - val_acc: 0.9802\n",
      "Epoch 175/600\n",
      " - 0s - loss: 0.0164 - acc: 0.9827 - val_loss: 0.0210 - val_acc: 0.9722\n",
      "Epoch 176/600\n",
      " - 0s - loss: 0.0148 - acc: 0.9852 - val_loss: 0.0174 - val_acc: 0.9800\n",
      "Epoch 177/600\n",
      " - 0s - loss: 0.0157 - acc: 0.9836 - val_loss: 0.0153 - val_acc: 0.9840\n",
      "Epoch 178/600\n",
      " - 0s - loss: 0.0142 - acc: 0.9865 - val_loss: 0.0197 - val_acc: 0.9782\n",
      "Epoch 179/600\n",
      " - 0s - loss: 0.0153 - acc: 0.9835 - val_loss: 0.0174 - val_acc: 0.9843\n",
      "Epoch 180/600\n",
      " - 0s - loss: 0.0146 - acc: 0.9853 - val_loss: 0.0162 - val_acc: 0.9807\n",
      "Epoch 181/600\n",
      " - 0s - loss: 0.0148 - acc: 0.9840 - val_loss: 0.0153 - val_acc: 0.9843\n",
      "Epoch 182/600\n",
      " - 0s - loss: 0.0146 - acc: 0.9848 - val_loss: 0.0157 - val_acc: 0.9850\n",
      "Epoch 183/600\n",
      " - 0s - loss: 0.0143 - acc: 0.9860 - val_loss: 0.0588 - val_acc: 0.9248\n",
      "Epoch 184/600\n",
      " - 0s - loss: 0.0214 - acc: 0.9755 - val_loss: 0.0183 - val_acc: 0.9767\n",
      "Epoch 185/600\n",
      " - 0s - loss: 0.0141 - acc: 0.9862 - val_loss: 0.0167 - val_acc: 0.9872\n",
      "Epoch 186/600\n",
      " - 0s - loss: 0.0144 - acc: 0.9866 - val_loss: 0.0147 - val_acc: 0.9843\n",
      "Epoch 187/600\n",
      " - 0s - loss: 0.0131 - acc: 0.9870 - val_loss: 0.0147 - val_acc: 0.9821\n",
      "Epoch 188/600\n",
      " - 0s - loss: 0.0138 - acc: 0.9849 - val_loss: 0.0292 - val_acc: 0.9667\n",
      "Epoch 189/600\n",
      " - 0s - loss: 0.0154 - acc: 0.9824 - val_loss: 0.0310 - val_acc: 0.9541\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 190/600\n",
      " - 0s - loss: 0.0172 - acc: 0.9815 - val_loss: 0.0168 - val_acc: 0.9891\n",
      "Epoch 191/600\n",
      " - 0s - loss: 0.0137 - acc: 0.9861 - val_loss: 0.0241 - val_acc: 0.9679\n",
      "Epoch 192/600\n",
      " - 0s - loss: 0.0158 - acc: 0.9829 - val_loss: 0.0169 - val_acc: 0.9838\n",
      "Epoch 193/600\n",
      " - 0s - loss: 0.0141 - acc: 0.9862 - val_loss: 0.0229 - val_acc: 0.9716\n",
      "Epoch 194/600\n",
      " - 0s - loss: 0.0155 - acc: 0.9832 - val_loss: 0.0237 - val_acc: 0.9683\n",
      "Epoch 195/600\n",
      " - 0s - loss: 0.0157 - acc: 0.9829 - val_loss: 0.0187 - val_acc: 0.9779\n",
      "Epoch 196/600\n",
      " - 0s - loss: 0.0147 - acc: 0.9851 - val_loss: 0.0172 - val_acc: 0.9783\n",
      "Epoch 197/600\n",
      " - 0s - loss: 0.0180 - acc: 0.9795 - val_loss: 0.0206 - val_acc: 0.9722\n",
      "Epoch 198/600\n",
      " - 0s - loss: 0.0139 - acc: 0.9861 - val_loss: 0.0153 - val_acc: 0.9857\n",
      "Epoch 199/600\n",
      " - 0s - loss: 0.0132 - acc: 0.9869 - val_loss: 0.0161 - val_acc: 0.9802\n",
      "Epoch 200/600\n",
      " - 0s - loss: 0.0144 - acc: 0.9854 - val_loss: 0.0169 - val_acc: 0.9807\n",
      "Epoch 201/600\n",
      " - 0s - loss: 0.0148 - acc: 0.9850 - val_loss: 0.0202 - val_acc: 0.9782\n",
      "Epoch 202/600\n",
      " - 0s - loss: 0.0157 - acc: 0.9830 - val_loss: 0.0147 - val_acc: 0.9913\n",
      "Epoch 203/600\n",
      " - 0s - loss: 0.0140 - acc: 0.9859 - val_loss: 0.0283 - val_acc: 0.9613\n",
      "Epoch 204/600\n",
      " - 0s - loss: 0.0155 - acc: 0.9832 - val_loss: 0.0154 - val_acc: 0.9867\n",
      "Epoch 205/600\n",
      " - 0s - loss: 0.0142 - acc: 0.9847 - val_loss: 0.0384 - val_acc: 0.9487\n",
      "Epoch 206/600\n",
      " - 0s - loss: 0.0189 - acc: 0.9781 - val_loss: 0.0280 - val_acc: 0.9592\n",
      "Epoch 207/600\n",
      " - 0s - loss: 0.0170 - acc: 0.9805 - val_loss: 0.0146 - val_acc: 0.9868\n",
      "Epoch 208/600\n",
      " - 0s - loss: 0.0148 - acc: 0.9851 - val_loss: 0.0156 - val_acc: 0.9807\n",
      "Epoch 209/600\n",
      " - 0s - loss: 0.0133 - acc: 0.9871 - val_loss: 0.0156 - val_acc: 0.9927\n",
      "Epoch 210/600\n",
      " - 0s - loss: 0.0129 - acc: 0.9864 - val_loss: 0.0179 - val_acc: 0.9768\n",
      "Epoch 211/600\n",
      " - 0s - loss: 0.0144 - acc: 0.9852 - val_loss: 0.0480 - val_acc: 0.9448\n",
      "Epoch 212/600\n",
      " - 0s - loss: 0.0246 - acc: 0.9720 - val_loss: 0.0305 - val_acc: 0.9676\n",
      "Epoch 213/600\n",
      " - 0s - loss: 0.0194 - acc: 0.9784 - val_loss: 0.0411 - val_acc: 0.9422\n",
      "Epoch 214/600\n",
      " - 0s - loss: 0.0204 - acc: 0.9766 - val_loss: 0.0154 - val_acc: 0.9801\n",
      "Epoch 215/600\n",
      " - 0s - loss: 0.0146 - acc: 0.9865 - val_loss: 0.0267 - val_acc: 0.9638\n",
      "Epoch 216/600\n",
      " - 0s - loss: 0.0155 - acc: 0.9849 - val_loss: 0.0172 - val_acc: 0.9771\n",
      "Epoch 217/600\n",
      " - 0s - loss: 0.0155 - acc: 0.9843 - val_loss: 0.0191 - val_acc: 0.9762\n",
      "Epoch 218/600\n",
      " - 0s - loss: 0.0144 - acc: 0.9860 - val_loss: 0.0151 - val_acc: 0.9804\n",
      "Epoch 219/600\n",
      " - 0s - loss: 0.0137 - acc: 0.9870 - val_loss: 0.0141 - val_acc: 0.9843\n",
      "Epoch 220/600\n",
      " - 0s - loss: 0.0134 - acc: 0.9879 - val_loss: 0.0363 - val_acc: 0.9507\n",
      "Epoch 221/600\n",
      " - 0s - loss: 0.0196 - acc: 0.9786 - val_loss: 0.0140 - val_acc: 0.9823\n",
      "Epoch 222/600\n",
      " - 0s - loss: 0.0130 - acc: 0.9879 - val_loss: 0.0254 - val_acc: 0.9660\n",
      "Epoch 223/600\n",
      " - 0s - loss: 0.0157 - acc: 0.9824 - val_loss: 0.0150 - val_acc: 0.9768\n",
      "Epoch 224/600\n",
      " - 0s - loss: 0.0134 - acc: 0.9872 - val_loss: 0.0140 - val_acc: 0.9843\n",
      "Epoch 225/600\n",
      " - 0s - loss: 0.0134 - acc: 0.9870 - val_loss: 0.0232 - val_acc: 0.9699\n",
      "Epoch 226/600\n",
      " - 0s - loss: 0.0146 - acc: 0.9844 - val_loss: 0.0174 - val_acc: 0.9754\n",
      "Epoch 227/600\n",
      " - 0s - loss: 0.0157 - acc: 0.9829 - val_loss: 0.0191 - val_acc: 0.9754\n",
      "Epoch 228/600\n",
      " - 0s - loss: 0.0140 - acc: 0.9864 - val_loss: 0.0199 - val_acc: 0.9782\n",
      "Epoch 229/600\n",
      " - 0s - loss: 0.0131 - acc: 0.9881 - val_loss: 0.0398 - val_acc: 0.9472\n",
      "Epoch 230/600\n",
      " - 0s - loss: 0.0221 - acc: 0.9734 - val_loss: 0.0141 - val_acc: 0.9890\n",
      "Epoch 231/600\n",
      " - 0s - loss: 0.0142 - acc: 0.9861 - val_loss: 0.0182 - val_acc: 0.9841\n",
      "Epoch 232/600\n",
      " - 0s - loss: 0.0143 - acc: 0.9857 - val_loss: 0.0192 - val_acc: 0.9762\n",
      "Epoch 233/600\n",
      " - 0s - loss: 0.0135 - acc: 0.9867 - val_loss: 0.0136 - val_acc: 0.9839\n",
      "Epoch 234/600\n",
      " - 0s - loss: 0.0134 - acc: 0.9882 - val_loss: 0.0203 - val_acc: 0.9748\n",
      "Epoch 235/600\n",
      " - 0s - loss: 0.0172 - acc: 0.9822 - val_loss: 0.0163 - val_acc: 0.9877\n",
      "Epoch 236/600\n",
      " - 0s - loss: 0.0140 - acc: 0.9860 - val_loss: 0.0133 - val_acc: 0.9858\n",
      "Epoch 237/600\n",
      " - 0s - loss: 0.0131 - acc: 0.9873 - val_loss: 0.0186 - val_acc: 0.9760\n",
      "Epoch 238/600\n",
      " - 0s - loss: 0.0145 - acc: 0.9855 - val_loss: 0.0163 - val_acc: 0.9841\n",
      "Epoch 239/600\n",
      " - 0s - loss: 0.0131 - acc: 0.9870 - val_loss: 0.0135 - val_acc: 0.9862\n",
      "Epoch 240/600\n",
      " - 0s - loss: 0.0137 - acc: 0.9858 - val_loss: 0.0132 - val_acc: 0.9877\n",
      "Epoch 241/600\n",
      " - 0s - loss: 0.0126 - acc: 0.9879 - val_loss: 0.0150 - val_acc: 0.9886\n",
      "Epoch 242/600\n",
      " - 0s - loss: 0.0127 - acc: 0.9880 - val_loss: 0.0145 - val_acc: 0.9913\n",
      "Epoch 243/600\n",
      " - 0s - loss: 0.0123 - acc: 0.9882 - val_loss: 0.0133 - val_acc: 0.9877\n",
      "Epoch 244/600\n",
      " - 0s - loss: 0.0126 - acc: 0.9876 - val_loss: 0.0172 - val_acc: 0.9787\n",
      "Epoch 245/600\n",
      " - 0s - loss: 0.0126 - acc: 0.9875 - val_loss: 0.0136 - val_acc: 0.9913\n",
      "Epoch 246/600\n",
      " - 0s - loss: 0.0128 - acc: 0.9870 - val_loss: 0.0135 - val_acc: 0.9880\n",
      "Epoch 247/600\n",
      " - 0s - loss: 0.0125 - acc: 0.9880 - val_loss: 0.0143 - val_acc: 0.9927\n",
      "Epoch 248/600\n",
      " - 0s - loss: 0.0120 - acc: 0.9887 - val_loss: 0.0157 - val_acc: 0.9804\n",
      "Epoch 249/600\n",
      " - 0s - loss: 0.0131 - acc: 0.9878 - val_loss: 0.0133 - val_acc: 0.9894\n",
      "Epoch 250/600\n",
      " - 0s - loss: 0.0122 - acc: 0.9886 - val_loss: 0.0137 - val_acc: 0.9890\n",
      "Epoch 251/600\n",
      " - 0s - loss: 0.0128 - acc: 0.9868 - val_loss: 0.0201 - val_acc: 0.9801\n",
      "Epoch 252/600\n",
      " - 0s - loss: 0.0145 - acc: 0.9843 - val_loss: 0.0143 - val_acc: 0.9876\n",
      "Epoch 253/600\n",
      " - 0s - loss: 0.0124 - acc: 0.9881 - val_loss: 0.0131 - val_acc: 0.9894\n",
      "Epoch 254/600\n",
      " - 0s - loss: 0.0126 - acc: 0.9880 - val_loss: 0.0192 - val_acc: 0.9809\n",
      "Epoch 255/600\n",
      " - 0s - loss: 0.0181 - acc: 0.9778 - val_loss: 0.0324 - val_acc: 0.9609\n",
      "Epoch 256/600\n",
      " - 0s - loss: 0.0207 - acc: 0.9751 - val_loss: 0.0185 - val_acc: 0.9756\n",
      "Epoch 257/600\n",
      " - 0s - loss: 0.0179 - acc: 0.9771 - val_loss: 0.0174 - val_acc: 0.9750\n",
      "Epoch 258/600\n",
      " - 0s - loss: 0.0165 - acc: 0.9820 - val_loss: 0.0180 - val_acc: 0.9727\n",
      "Epoch 259/600\n",
      " - 0s - loss: 0.0170 - acc: 0.9799 - val_loss: 0.0275 - val_acc: 0.9609\n",
      "Epoch 260/600\n",
      " - 0s - loss: 0.0194 - acc: 0.9755 - val_loss: 0.0189 - val_acc: 0.9747\n",
      "Epoch 261/600\n",
      " - 0s - loss: 0.0172 - acc: 0.9783 - val_loss: 0.0176 - val_acc: 0.9786\n",
      "Epoch 262/600\n",
      " - 0s - loss: 0.0172 - acc: 0.9789 - val_loss: 0.0173 - val_acc: 0.9843\n",
      "Epoch 263/600\n",
      " - 0s - loss: 0.0167 - acc: 0.9805 - val_loss: 0.0169 - val_acc: 0.9804\n",
      "Epoch 264/600\n",
      " - 0s - loss: 0.0161 - acc: 0.9829 - val_loss: 0.0167 - val_acc: 0.9804\n",
      "Epoch 265/600\n",
      " - 0s - loss: 0.0168 - acc: 0.9801 - val_loss: 0.0200 - val_acc: 0.9723\n",
      "Epoch 266/600\n",
      " - 0s - loss: 0.0170 - acc: 0.9809 - val_loss: 0.0166 - val_acc: 0.9823\n",
      "Epoch 267/600\n",
      " - 0s - loss: 0.0161 - acc: 0.9824 - val_loss: 0.0171 - val_acc: 0.9727\n",
      "Epoch 268/600\n",
      " - 0s - loss: 0.0166 - acc: 0.9807 - val_loss: 0.0166 - val_acc: 0.9854\n",
      "Epoch 269/600\n",
      " - 0s - loss: 0.0179 - acc: 0.9777 - val_loss: 0.0203 - val_acc: 0.9728\n",
      "Epoch 270/600\n",
      " - 0s - loss: 0.0164 - acc: 0.9802 - val_loss: 0.0205 - val_acc: 0.9718\n",
      "Epoch 271/600\n",
      " - 0s - loss: 0.0174 - acc: 0.9785 - val_loss: 0.0196 - val_acc: 0.9721\n",
      "Epoch 272/600\n",
      " - 0s - loss: 0.0185 - acc: 0.9758 - val_loss: 0.0227 - val_acc: 0.9699\n",
      "Epoch 273/600\n",
      " - 0s - loss: 0.0173 - acc: 0.9793 - val_loss: 0.0164 - val_acc: 0.9847\n",
      "Epoch 274/600\n",
      " - 0s - loss: 0.0158 - acc: 0.9816 - val_loss: 0.0187 - val_acc: 0.9742\n",
      "Epoch 275/600\n",
      " - 0s - loss: 0.0159 - acc: 0.9820 - val_loss: 0.0457 - val_acc: 0.9460\n",
      "Epoch 276/600\n",
      " - 0s - loss: 0.0213 - acc: 0.9740 - val_loss: 0.0250 - val_acc: 0.9673\n",
      "Epoch 277/600\n",
      " - 0s - loss: 0.0168 - acc: 0.9789 - val_loss: 0.0815 - val_acc: 0.9060\n",
      "Epoch 278/600\n",
      " - 0s - loss: 0.0368 - acc: 0.9569 - val_loss: 0.0479 - val_acc: 0.9457\n",
      "Epoch 279/600\n",
      " - 0s - loss: 0.0246 - acc: 0.9710 - val_loss: 0.0420 - val_acc: 0.9380\n",
      "Epoch 280/600\n",
      " - 0s - loss: 0.0220 - acc: 0.9723 - val_loss: 0.0213 - val_acc: 0.9757\n",
      "Epoch 281/600\n",
      " - 0s - loss: 0.0195 - acc: 0.9756 - val_loss: 0.0542 - val_acc: 0.9182\n",
      "Epoch 282/600\n",
      " - 0s - loss: 0.0244 - acc: 0.9713 - val_loss: 0.0357 - val_acc: 0.9569\n",
      "Epoch 283/600\n",
      " - 0s - loss: 0.0242 - acc: 0.9699 - val_loss: 0.0169 - val_acc: 0.9843\n",
      "Epoch 284/600\n",
      " - 0s - loss: 0.0158 - acc: 0.9807 - val_loss: 0.0244 - val_acc: 0.9709\n",
      "Epoch 285/600\n",
      " - 0s - loss: 0.0190 - acc: 0.9772 - val_loss: 0.0219 - val_acc: 0.9711\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 286/600\n",
      " - 0s - loss: 0.0169 - acc: 0.9789 - val_loss: 0.0317 - val_acc: 0.9534\n",
      "Epoch 287/600\n",
      " - 0s - loss: 0.0204 - acc: 0.9765 - val_loss: 0.0271 - val_acc: 0.9603\n",
      "Epoch 288/600\n",
      " - 0s - loss: 0.0169 - acc: 0.9790 - val_loss: 0.0160 - val_acc: 0.9823\n",
      "Epoch 289/600\n",
      " - 0s - loss: 0.0158 - acc: 0.9805 - val_loss: 0.0160 - val_acc: 0.9891\n",
      "Epoch 290/600\n",
      " - 0s - loss: 0.0151 - acc: 0.9847 - val_loss: 0.0237 - val_acc: 0.9657\n",
      "Epoch 291/600\n",
      " - 0s - loss: 0.0168 - acc: 0.9783 - val_loss: 0.0180 - val_acc: 0.9746\n",
      "Epoch 292/600\n",
      " - 0s - loss: 0.0153 - acc: 0.9838 - val_loss: 0.0182 - val_acc: 0.9728\n",
      "Epoch 293/600\n",
      " - 0s - loss: 0.0157 - acc: 0.9818 - val_loss: 0.0203 - val_acc: 0.9742\n",
      "Epoch 294/600\n",
      " - 0s - loss: 0.0162 - acc: 0.9799 - val_loss: 0.0195 - val_acc: 0.9773\n",
      "Epoch 295/600\n",
      " - 0s - loss: 0.0176 - acc: 0.9776 - val_loss: 0.0162 - val_acc: 0.9786\n",
      "Epoch 296/600\n",
      " - 0s - loss: 0.0151 - acc: 0.9830 - val_loss: 0.0581 - val_acc: 0.9239\n",
      "Epoch 297/600\n",
      " - 0s - loss: 0.0288 - acc: 0.9628 - val_loss: 0.0367 - val_acc: 0.9584\n",
      "Epoch 298/600\n",
      " - 0s - loss: 0.0219 - acc: 0.9730 - val_loss: 0.0175 - val_acc: 0.9789\n",
      "Epoch 299/600\n",
      " - 0s - loss: 0.0165 - acc: 0.9788 - val_loss: 0.0188 - val_acc: 0.9757\n",
      "Epoch 300/600\n",
      " - 0s - loss: 0.0165 - acc: 0.9796 - val_loss: 0.0177 - val_acc: 0.9807\n",
      "Epoch 301/600\n",
      " - 0s - loss: 0.0170 - acc: 0.9791 - val_loss: 0.0168 - val_acc: 0.9829\n",
      "Epoch 302/600\n",
      " - 0s - loss: 0.0163 - acc: 0.9805 - val_loss: 0.0269 - val_acc: 0.9640\n",
      "Epoch 303/600\n",
      " - 0s - loss: 0.0219 - acc: 0.9715 - val_loss: 0.0290 - val_acc: 0.9614\n",
      "Epoch 304/600\n",
      " - 0s - loss: 0.0227 - acc: 0.9720 - val_loss: 0.0182 - val_acc: 0.9824\n",
      "Epoch 305/600\n",
      " - 0s - loss: 0.0163 - acc: 0.9807 - val_loss: 0.0186 - val_acc: 0.9767\n",
      "Epoch 306/600\n",
      " - 0s - loss: 0.0162 - acc: 0.9803 - val_loss: 0.0201 - val_acc: 0.9729\n",
      "Epoch 307/600\n",
      " - 0s - loss: 0.0164 - acc: 0.9801 - val_loss: 0.0199 - val_acc: 0.9754\n",
      "Epoch 308/600\n",
      " - 0s - loss: 0.0162 - acc: 0.9801 - val_loss: 0.0231 - val_acc: 0.9708\n",
      "Epoch 309/600\n",
      " - 0s - loss: 0.0172 - acc: 0.9783 - val_loss: 0.0360 - val_acc: 0.9541\n",
      "Epoch 310/600\n",
      " - 0s - loss: 0.0199 - acc: 0.9769 - val_loss: 0.0175 - val_acc: 0.9727\n",
      "Epoch 311/600\n",
      " - 0s - loss: 0.0157 - acc: 0.9818 - val_loss: 0.0164 - val_acc: 0.9772\n",
      "Epoch 312/600\n",
      " - 0s - loss: 0.0155 - acc: 0.9821 - val_loss: 0.0277 - val_acc: 0.9612\n",
      "Epoch 313/600\n",
      " - 0s - loss: 0.0186 - acc: 0.9761 - val_loss: 0.0171 - val_acc: 0.9786\n",
      "Epoch 314/600\n",
      " - 0s - loss: 0.0155 - acc: 0.9833 - val_loss: 0.0249 - val_acc: 0.9683\n",
      "Epoch 315/600\n",
      " - 0s - loss: 0.0170 - acc: 0.9811 - val_loss: 0.0158 - val_acc: 0.9842\n",
      "Epoch 316/600\n",
      " - 0s - loss: 0.0155 - acc: 0.9821 - val_loss: 0.0163 - val_acc: 0.9809\n",
      "Epoch 317/600\n",
      " - 0s - loss: 0.0149 - acc: 0.9831 - val_loss: 0.0200 - val_acc: 0.9722\n",
      "Epoch 318/600\n",
      " - 0s - loss: 0.0163 - acc: 0.9794 - val_loss: 0.0228 - val_acc: 0.9679\n",
      "Epoch 319/600\n",
      " - 0s - loss: 0.0170 - acc: 0.9787 - val_loss: 0.0162 - val_acc: 0.9786\n",
      "Epoch 320/600\n",
      " - 0s - loss: 0.0156 - acc: 0.9823 - val_loss: 0.0162 - val_acc: 0.9809\n",
      "Epoch 321/600\n",
      " - 0s - loss: 0.0153 - acc: 0.9823 - val_loss: 0.0162 - val_acc: 0.9764\n",
      "Epoch 322/600\n",
      " - 0s - loss: 0.0153 - acc: 0.9821 - val_loss: 0.0163 - val_acc: 0.9859\n",
      "Epoch 323/600\n",
      " - 0s - loss: 0.0149 - acc: 0.9833 - val_loss: 0.0159 - val_acc: 0.9841\n",
      "Epoch 324/600\n",
      " - 0s - loss: 0.0149 - acc: 0.9840 - val_loss: 0.0155 - val_acc: 0.9859\n",
      "Epoch 325/600\n",
      " - 0s - loss: 0.0149 - acc: 0.9832 - val_loss: 0.0155 - val_acc: 0.9842\n",
      "Epoch 326/600\n",
      " - 0s - loss: 0.0149 - acc: 0.9831 - val_loss: 0.0179 - val_acc: 0.9797\n",
      "Epoch 327/600\n",
      " - 0s - loss: 0.0151 - acc: 0.9825 - val_loss: 0.0253 - val_acc: 0.9647\n",
      "Epoch 328/600\n",
      " - 0s - loss: 0.0161 - acc: 0.9802 - val_loss: 0.0454 - val_acc: 0.9497\n",
      "Epoch 329/600\n",
      " - 0s - loss: 0.0233 - acc: 0.9718 - val_loss: 0.0218 - val_acc: 0.9740\n",
      "Epoch 330/600\n",
      " - 0s - loss: 0.0161 - acc: 0.9797 - val_loss: 0.0187 - val_acc: 0.9718\n",
      "Epoch 331/600\n",
      " - 0s - loss: 0.0162 - acc: 0.9788 - val_loss: 0.0159 - val_acc: 0.9823\n",
      "Epoch 332/600\n",
      " - 0s - loss: 0.0154 - acc: 0.9812 - val_loss: 0.0175 - val_acc: 0.9764\n",
      "Epoch 333/600\n",
      " - 0s - loss: 0.0151 - acc: 0.9827 - val_loss: 0.0171 - val_acc: 0.9727\n",
      "Epoch 334/600\n",
      " - 0s - loss: 0.0158 - acc: 0.9816 - val_loss: 0.0158 - val_acc: 0.9809\n",
      "Epoch 335/600\n",
      " - 0s - loss: 0.0158 - acc: 0.9805 - val_loss: 0.0153 - val_acc: 0.9859\n",
      "Epoch 336/600\n",
      " - 0s - loss: 0.0162 - acc: 0.9791 - val_loss: 0.0158 - val_acc: 0.9822\n",
      "Epoch 337/600\n",
      " - 0s - loss: 0.0145 - acc: 0.9847 - val_loss: 0.0153 - val_acc: 0.9808\n",
      "Epoch 338/600\n",
      " - 0s - loss: 0.0151 - acc: 0.9821 - val_loss: 0.0265 - val_acc: 0.9674\n",
      "Epoch 339/600\n",
      " - 0s - loss: 0.0156 - acc: 0.9805 - val_loss: 0.0158 - val_acc: 0.9790\n",
      "Epoch 340/600\n",
      " - 0s - loss: 0.0150 - acc: 0.9822 - val_loss: 0.0163 - val_acc: 0.9746\n",
      "Epoch 341/600\n",
      " - 0s - loss: 0.0147 - acc: 0.9836 - val_loss: 0.0155 - val_acc: 0.9827\n",
      "Epoch 342/600\n",
      " - 0s - loss: 0.0146 - acc: 0.9831 - val_loss: 0.0159 - val_acc: 0.9807\n",
      "Epoch 343/600\n",
      " - 0s - loss: 0.0147 - acc: 0.9835 - val_loss: 0.0156 - val_acc: 0.9827\n",
      "Epoch 344/600\n",
      " - 0s - loss: 0.0143 - acc: 0.9845 - val_loss: 0.0602 - val_acc: 0.9211\n",
      "Epoch 345/600\n",
      " - 0s - loss: 0.0273 - acc: 0.9667 - val_loss: 0.0160 - val_acc: 0.9809\n",
      "Epoch 346/600\n",
      " - 0s - loss: 0.0165 - acc: 0.9798 - val_loss: 0.0383 - val_acc: 0.9451\n",
      "Epoch 347/600\n",
      " - 0s - loss: 0.0214 - acc: 0.9756 - val_loss: 0.0162 - val_acc: 0.9823\n",
      "Epoch 348/600\n",
      " - 0s - loss: 0.0148 - acc: 0.9823 - val_loss: 0.0161 - val_acc: 0.9786\n",
      "Epoch 349/600\n",
      " - 0s - loss: 0.0142 - acc: 0.9853 - val_loss: 0.0178 - val_acc: 0.9767\n",
      "Epoch 350/600\n",
      " - 0s - loss: 0.0151 - acc: 0.9823 - val_loss: 0.0170 - val_acc: 0.9788\n",
      "Epoch 351/600\n",
      " - 0s - loss: 0.0163 - acc: 0.9805 - val_loss: 0.0156 - val_acc: 0.9840\n",
      "Epoch 352/600\n",
      " - 0s - loss: 0.0146 - acc: 0.9833 - val_loss: 0.0169 - val_acc: 0.9788\n",
      "Epoch 353/600\n",
      " - 0s - loss: 0.0151 - acc: 0.9813 - val_loss: 0.0167 - val_acc: 0.9746\n",
      "Epoch 354/600\n",
      " - 0s - loss: 0.0147 - acc: 0.9828 - val_loss: 0.0180 - val_acc: 0.9727\n",
      "Epoch 355/600\n",
      " - 0s - loss: 0.0148 - acc: 0.9827 - val_loss: 0.0167 - val_acc: 0.9764\n",
      "Epoch 356/600\n",
      " - 0s - loss: 0.0152 - acc: 0.9806 - val_loss: 0.0159 - val_acc: 0.9800\n",
      "Epoch 357/600\n",
      " - 0s - loss: 0.0147 - acc: 0.9830 - val_loss: 0.0161 - val_acc: 0.9769\n",
      "Epoch 358/600\n",
      " - 0s - loss: 0.0146 - acc: 0.9839 - val_loss: 0.0150 - val_acc: 0.9840\n",
      "Epoch 359/600\n",
      " - 0s - loss: 0.0142 - acc: 0.9849 - val_loss: 0.0393 - val_acc: 0.9542\n",
      "Epoch 360/600\n",
      " - 0s - loss: 0.0234 - acc: 0.9716 - val_loss: 0.0162 - val_acc: 0.9786\n",
      "Epoch 361/600\n",
      " - 0s - loss: 0.0147 - acc: 0.9824 - val_loss: 0.0165 - val_acc: 0.9764\n",
      "Epoch 362/600\n",
      " - 0s - loss: 0.0146 - acc: 0.9832 - val_loss: 0.0166 - val_acc: 0.9764\n",
      "Epoch 363/600\n",
      " - 0s - loss: 0.0144 - acc: 0.9846 - val_loss: 0.0164 - val_acc: 0.9746\n",
      "Epoch 364/600\n",
      " - 0s - loss: 0.0150 - acc: 0.9823 - val_loss: 0.0148 - val_acc: 0.9859\n",
      "Epoch 365/600\n",
      " - 0s - loss: 0.0141 - acc: 0.9846 - val_loss: 0.0401 - val_acc: 0.9521\n",
      "Epoch 366/600\n",
      " - 0s - loss: 0.0200 - acc: 0.9754 - val_loss: 0.0276 - val_acc: 0.9588\n",
      "Epoch 367/600\n",
      " - 0s - loss: 0.0184 - acc: 0.9772 - val_loss: 0.0156 - val_acc: 0.9840\n",
      "Epoch 368/600\n",
      " - 0s - loss: 0.0141 - acc: 0.9846 - val_loss: 0.0152 - val_acc: 0.9790\n",
      "Epoch 369/600\n",
      " - 0s - loss: 0.0142 - acc: 0.9838 - val_loss: 0.0163 - val_acc: 0.9764\n",
      "Epoch 370/600\n",
      " - 0s - loss: 0.0143 - acc: 0.9840 - val_loss: 0.0154 - val_acc: 0.9821\n",
      "Epoch 371/600\n",
      " - 0s - loss: 0.0142 - acc: 0.9840 - val_loss: 0.0176 - val_acc: 0.9746\n",
      "Epoch 372/600\n",
      " - 0s - loss: 0.0145 - acc: 0.9839 - val_loss: 0.0158 - val_acc: 0.9807\n",
      "Epoch 373/600\n",
      " - 0s - loss: 0.0146 - acc: 0.9833 - val_loss: 0.0735 - val_acc: 0.9204\n",
      "Epoch 374/600\n",
      " - 0s - loss: 0.0270 - acc: 0.9701 - val_loss: 0.0280 - val_acc: 0.9552\n",
      "Epoch 375/600\n",
      " - 0s - loss: 0.0171 - acc: 0.9779 - val_loss: 0.0405 - val_acc: 0.9438\n",
      "Epoch 376/600\n",
      " - 0s - loss: 0.0249 - acc: 0.9707 - val_loss: 0.0167 - val_acc: 0.9746\n",
      "Epoch 377/600\n",
      " - 0s - loss: 0.0144 - acc: 0.9832 - val_loss: 0.0147 - val_acc: 0.9859\n",
      "Epoch 378/600\n",
      " - 0s - loss: 0.0141 - acc: 0.9853 - val_loss: 0.0147 - val_acc: 0.9859\n",
      "Epoch 379/600\n",
      " - 0s - loss: 0.0142 - acc: 0.9846 - val_loss: 0.0281 - val_acc: 0.9613\n",
      "Epoch 380/600\n",
      " - 0s - loss: 0.0166 - acc: 0.9798 - val_loss: 0.0151 - val_acc: 0.9844\n",
      "Epoch 381/600\n",
      " - 0s - loss: 0.0150 - acc: 0.9823 - val_loss: 0.0432 - val_acc: 0.9439\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 382/600\n",
      " - 0s - loss: 0.0211 - acc: 0.9736 - val_loss: 0.0160 - val_acc: 0.9786\n",
      "Epoch 383/600\n",
      " - 0s - loss: 0.0146 - acc: 0.9829 - val_loss: 0.0149 - val_acc: 0.9809\n",
      "Epoch 384/600\n",
      " - 0s - loss: 0.0145 - acc: 0.9837 - val_loss: 0.0262 - val_acc: 0.9686\n",
      "Epoch 385/600\n",
      " - 0s - loss: 0.0164 - acc: 0.9823 - val_loss: 0.0291 - val_acc: 0.9623\n",
      "Epoch 386/600\n",
      " - 0s - loss: 0.0167 - acc: 0.9793 - val_loss: 0.0391 - val_acc: 0.9508\n",
      "Epoch 387/600\n",
      " - 0s - loss: 0.0242 - acc: 0.9731 - val_loss: 0.0151 - val_acc: 0.9836\n",
      "Epoch 388/600\n",
      " - 0s - loss: 0.0140 - acc: 0.9845 - val_loss: 0.0157 - val_acc: 0.9783\n",
      "Epoch 389/600\n",
      " - 0s - loss: 0.0142 - acc: 0.9836 - val_loss: 0.0157 - val_acc: 0.9764\n",
      "Epoch 390/600\n",
      " - 0s - loss: 0.0144 - acc: 0.9822 - val_loss: 0.0165 - val_acc: 0.9764\n",
      "Epoch 391/600\n",
      " - 0s - loss: 0.0141 - acc: 0.9847 - val_loss: 0.0243 - val_acc: 0.9674\n",
      "Epoch 392/600\n",
      " - 0s - loss: 0.0159 - acc: 0.9810 - val_loss: 0.0157 - val_acc: 0.9803\n",
      "Epoch 393/600\n",
      " - 0s - loss: 0.0145 - acc: 0.9836 - val_loss: 0.0162 - val_acc: 0.9764\n",
      "Epoch 394/600\n",
      " - 0s - loss: 0.0138 - acc: 0.9859 - val_loss: 0.0547 - val_acc: 0.9392\n",
      "Epoch 395/600\n",
      " - 0s - loss: 0.0285 - acc: 0.9689 - val_loss: 0.0206 - val_acc: 0.9850\n",
      "Epoch 396/600\n",
      " - 0s - loss: 0.0177 - acc: 0.9840 - val_loss: 0.0197 - val_acc: 0.9818\n",
      "Epoch 397/600\n",
      " - 0s - loss: 0.0176 - acc: 0.9832 - val_loss: 0.0461 - val_acc: 0.9467\n",
      "Epoch 398/600\n",
      " - 0s - loss: 0.0340 - acc: 0.9645 - val_loss: 0.0419 - val_acc: 0.9510\n",
      "Epoch 399/600\n",
      " - 0s - loss: 0.0213 - acc: 0.9743 - val_loss: 0.0213 - val_acc: 0.9744\n",
      "Epoch 400/600\n",
      " - 0s - loss: 0.0169 - acc: 0.9838 - val_loss: 0.0182 - val_acc: 0.9837\n",
      "Epoch 401/600\n",
      " - 0s - loss: 0.0163 - acc: 0.9855 - val_loss: 0.0243 - val_acc: 0.9650\n",
      "Epoch 402/600\n",
      " - 0s - loss: 0.0170 - acc: 0.9829 - val_loss: 0.0223 - val_acc: 0.9696\n",
      "Epoch 403/600\n",
      " - 0s - loss: 0.0172 - acc: 0.9817 - val_loss: 0.0221 - val_acc: 0.9693\n",
      "Epoch 404/600\n",
      " - 0s - loss: 0.0216 - acc: 0.9733 - val_loss: 0.0209 - val_acc: 0.9728\n",
      "Epoch 405/600\n",
      " - 0s - loss: 0.0168 - acc: 0.9821 - val_loss: 0.0181 - val_acc: 0.9799\n",
      "Epoch 406/600\n",
      " - 0s - loss: 0.0160 - acc: 0.9850 - val_loss: 0.0232 - val_acc: 0.9681\n",
      "Epoch 407/600\n",
      " - 0s - loss: 0.0210 - acc: 0.9756 - val_loss: 0.0185 - val_acc: 0.9798\n",
      "Epoch 408/600\n",
      " - 0s - loss: 0.0158 - acc: 0.9843 - val_loss: 0.0200 - val_acc: 0.9774\n",
      "Epoch 409/600\n",
      " - 0s - loss: 0.0159 - acc: 0.9846 - val_loss: 0.0186 - val_acc: 0.9761\n",
      "Epoch 410/600\n",
      " - 0s - loss: 0.0156 - acc: 0.9848 - val_loss: 0.0203 - val_acc: 0.9708\n",
      "Epoch 411/600\n",
      " - 0s - loss: 0.0165 - acc: 0.9822 - val_loss: 0.0188 - val_acc: 0.9781\n",
      "Epoch 412/600\n",
      " - 0s - loss: 0.0167 - acc: 0.9819 - val_loss: 0.0173 - val_acc: 0.9792\n",
      "Epoch 413/600\n",
      " - 0s - loss: 0.0154 - acc: 0.9852 - val_loss: 0.0167 - val_acc: 0.9869\n",
      "Epoch 414/600\n",
      " - 0s - loss: 0.0154 - acc: 0.9856 - val_loss: 0.0174 - val_acc: 0.9850\n",
      "Epoch 415/600\n",
      " - 0s - loss: 0.0156 - acc: 0.9852 - val_loss: 0.0352 - val_acc: 0.9614\n",
      "Epoch 416/600\n",
      " - 0s - loss: 0.0193 - acc: 0.9794 - val_loss: 0.0233 - val_acc: 0.9646\n",
      "Epoch 417/600\n",
      " - 0s - loss: 0.0173 - acc: 0.9810 - val_loss: 0.0168 - val_acc: 0.9869\n",
      "Epoch 418/600\n",
      " - 0s - loss: 0.0157 - acc: 0.9829 - val_loss: 0.0195 - val_acc: 0.9742\n",
      "Epoch 419/600\n",
      " - 0s - loss: 0.0160 - acc: 0.9820 - val_loss: 0.0203 - val_acc: 0.9739\n",
      "Epoch 420/600\n",
      " - 0s - loss: 0.0160 - acc: 0.9835 - val_loss: 0.0166 - val_acc: 0.9827\n",
      "Epoch 421/600\n",
      " - 0s - loss: 0.0146 - acc: 0.9874 - val_loss: 0.0180 - val_acc: 0.9781\n",
      "Epoch 422/600\n",
      " - 0s - loss: 0.0154 - acc: 0.9847 - val_loss: 0.0203 - val_acc: 0.9689\n",
      "Epoch 423/600\n",
      " - 0s - loss: 0.0193 - acc: 0.9765 - val_loss: 0.0194 - val_acc: 0.9680\n",
      "Epoch 424/600\n",
      " - 0s - loss: 0.0150 - acc: 0.9844 - val_loss: 0.0170 - val_acc: 0.9803\n",
      "Epoch 425/600\n",
      " - 0s - loss: 0.0149 - acc: 0.9867 - val_loss: 0.0161 - val_acc: 0.9829\n",
      "Epoch 426/600\n",
      " - 0s - loss: 0.0150 - acc: 0.9852 - val_loss: 0.0160 - val_acc: 0.9829\n",
      "Epoch 427/600\n",
      " - 0s - loss: 0.0145 - acc: 0.9863 - val_loss: 0.0192 - val_acc: 0.9708\n",
      "Epoch 428/600\n",
      " - 0s - loss: 0.0148 - acc: 0.9845 - val_loss: 0.0196 - val_acc: 0.9690\n",
      "Epoch 429/600\n",
      " - 0s - loss: 0.0153 - acc: 0.9825 - val_loss: 0.0685 - val_acc: 0.9180\n",
      "Epoch 430/600\n",
      " - 0s - loss: 0.0290 - acc: 0.9678 - val_loss: 0.0169 - val_acc: 0.9846\n",
      "Epoch 431/600\n",
      " - 0s - loss: 0.0151 - acc: 0.9856 - val_loss: 0.0170 - val_acc: 0.9848\n",
      "Epoch 432/600\n",
      " - 0s - loss: 0.0148 - acc: 0.9862 - val_loss: 0.0164 - val_acc: 0.9848\n",
      "Epoch 433/600\n",
      " - 0s - loss: 0.0149 - acc: 0.9861 - val_loss: 0.0165 - val_acc: 0.9867\n",
      "Epoch 434/600\n",
      " - 0s - loss: 0.0147 - acc: 0.9860 - val_loss: 0.0166 - val_acc: 0.9824\n",
      "Epoch 435/600\n",
      " - 0s - loss: 0.0148 - acc: 0.9852 - val_loss: 0.0521 - val_acc: 0.9383\n",
      "Epoch 436/600\n",
      " - 0s - loss: 0.0270 - acc: 0.9686 - val_loss: 0.0165 - val_acc: 0.9799\n",
      "Epoch 437/600\n",
      " - 0s - loss: 0.0146 - acc: 0.9853 - val_loss: 0.0231 - val_acc: 0.9690\n",
      "Epoch 438/600\n",
      " - 0s - loss: 0.0160 - acc: 0.9826 - val_loss: 0.0253 - val_acc: 0.9678\n",
      "Epoch 439/600\n",
      " - 0s - loss: 0.0170 - acc: 0.9803 - val_loss: 0.0168 - val_acc: 0.9867\n",
      "Epoch 440/600\n",
      " - 0s - loss: 0.0149 - acc: 0.9844 - val_loss: 0.0177 - val_acc: 0.9781\n",
      "Epoch 441/600\n",
      " - 0s - loss: 0.0144 - acc: 0.9860 - val_loss: 0.0163 - val_acc: 0.9848\n",
      "Epoch 442/600\n",
      " - 0s - loss: 0.0144 - acc: 0.9862 - val_loss: 0.0312 - val_acc: 0.9654\n",
      "Epoch 443/600\n",
      " - 0s - loss: 0.0179 - acc: 0.9794 - val_loss: 0.0177 - val_acc: 0.9814\n",
      "Epoch 444/600\n",
      " - 0s - loss: 0.0153 - acc: 0.9840 - val_loss: 0.0409 - val_acc: 0.9613\n",
      "Epoch 445/600\n",
      " - 0s - loss: 0.0193 - acc: 0.9782 - val_loss: 0.0292 - val_acc: 0.9549\n",
      "Epoch 446/600\n",
      " - 0s - loss: 0.0209 - acc: 0.9752 - val_loss: 0.0165 - val_acc: 0.9834\n",
      "Epoch 447/600\n",
      " - 0s - loss: 0.0151 - acc: 0.9848 - val_loss: 0.0162 - val_acc: 0.9848\n",
      "Epoch 448/600\n",
      " - 0s - loss: 0.0143 - acc: 0.9868 - val_loss: 0.0163 - val_acc: 0.9813\n",
      "Epoch 449/600\n",
      " - 0s - loss: 0.0144 - acc: 0.9862 - val_loss: 0.0166 - val_acc: 0.9824\n",
      "Epoch 450/600\n",
      " - 0s - loss: 0.0146 - acc: 0.9847 - val_loss: 0.0165 - val_acc: 0.9867\n",
      "Epoch 451/600\n",
      " - 0s - loss: 0.0148 - acc: 0.9845 - val_loss: 0.0165 - val_acc: 0.9867\n",
      "Epoch 452/600\n",
      " - 0s - loss: 0.0147 - acc: 0.9848 - val_loss: 0.0283 - val_acc: 0.9633\n",
      "Epoch 453/600\n",
      " - 0s - loss: 0.0167 - acc: 0.9815 - val_loss: 0.0174 - val_acc: 0.9832\n",
      "Epoch 454/600\n",
      " - 0s - loss: 0.0149 - acc: 0.9851 - val_loss: 0.0186 - val_acc: 0.9759\n",
      "Epoch 455/600\n",
      " - 0s - loss: 0.0148 - acc: 0.9843 - val_loss: 0.0271 - val_acc: 0.9657\n",
      "Epoch 456/600\n",
      " - 0s - loss: 0.0165 - acc: 0.9825 - val_loss: 0.0183 - val_acc: 0.9781\n",
      "Epoch 457/600\n",
      " - 0s - loss: 0.0147 - acc: 0.9862 - val_loss: 0.0240 - val_acc: 0.9674\n",
      "Epoch 458/600\n",
      " - 0s - loss: 0.0179 - acc: 0.9785 - val_loss: 0.0189 - val_acc: 0.9742\n",
      "Epoch 459/600\n",
      " - 0s - loss: 0.0160 - acc: 0.9820 - val_loss: 0.0172 - val_acc: 0.9740\n",
      "Epoch 460/600\n",
      " - 0s - loss: 0.0147 - acc: 0.9840 - val_loss: 0.0174 - val_acc: 0.9833\n",
      "Epoch 461/600\n",
      " - 0s - loss: 0.0146 - acc: 0.9851 - val_loss: 0.0161 - val_acc: 0.9867\n",
      "Epoch 462/600\n",
      " - 0s - loss: 0.0142 - acc: 0.9867 - val_loss: 0.0159 - val_acc: 0.9848\n",
      "Epoch 463/600\n",
      " - 0s - loss: 0.0145 - acc: 0.9850 - val_loss: 0.0171 - val_acc: 0.9833\n",
      "Epoch 464/600\n",
      " - 0s - loss: 0.0147 - acc: 0.9850 - val_loss: 0.0166 - val_acc: 0.9818\n",
      "Epoch 465/600\n",
      " - 0s - loss: 0.0142 - acc: 0.9858 - val_loss: 0.0187 - val_acc: 0.9814\n",
      "Epoch 466/600\n",
      " - 0s - loss: 0.0146 - acc: 0.9858 - val_loss: 0.0158 - val_acc: 0.9848\n",
      "Epoch 467/600\n",
      " - 0s - loss: 0.0148 - acc: 0.9841 - val_loss: 0.0162 - val_acc: 0.9848\n",
      "Epoch 468/600\n",
      " - 0s - loss: 0.0142 - acc: 0.9864 - val_loss: 0.0171 - val_acc: 0.9804\n",
      "Epoch 469/600\n",
      " - 0s - loss: 0.0148 - acc: 0.9841 - val_loss: 0.0158 - val_acc: 0.9824\n",
      "Epoch 470/600\n",
      " - 0s - loss: 0.0142 - acc: 0.9861 - val_loss: 0.0191 - val_acc: 0.9712\n",
      "Epoch 471/600\n",
      " - 0s - loss: 0.0148 - acc: 0.9844 - val_loss: 0.0198 - val_acc: 0.9779\n",
      "Epoch 472/600\n",
      " - 0s - loss: 0.0158 - acc: 0.9815 - val_loss: 0.0210 - val_acc: 0.9743\n",
      "Epoch 473/600\n",
      " - 0s - loss: 0.0147 - acc: 0.9837 - val_loss: 0.0265 - val_acc: 0.9700\n",
      "Epoch 474/600\n",
      " - 0s - loss: 0.0168 - acc: 0.9797 - val_loss: 0.0169 - val_acc: 0.9824\n",
      "Epoch 475/600\n",
      " - 0s - loss: 0.0142 - acc: 0.9857 - val_loss: 0.0192 - val_acc: 0.9753\n",
      "Epoch 476/600\n",
      " - 0s - loss: 0.0144 - acc: 0.9850 - val_loss: 0.0158 - val_acc: 0.9867\n",
      "Epoch 477/600\n",
      " - 1s - loss: 0.0139 - acc: 0.9867 - val_loss: 0.0392 - val_acc: 0.9503\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 478/600\n",
      " - 0s - loss: 0.0181 - acc: 0.9790 - val_loss: 0.0182 - val_acc: 0.9833\n",
      "Epoch 479/600\n",
      " - 0s - loss: 0.0145 - acc: 0.9853 - val_loss: 0.0156 - val_acc: 0.9848\n",
      "Epoch 480/600\n",
      " - 0s - loss: 0.0151 - acc: 0.9830 - val_loss: 0.0343 - val_acc: 0.9503\n",
      "Epoch 481/600\n",
      " - 0s - loss: 0.0181 - acc: 0.9793 - val_loss: 0.0157 - val_acc: 0.9827\n",
      "Epoch 482/600\n",
      " - 0s - loss: 0.0145 - acc: 0.9847 - val_loss: 0.0179 - val_acc: 0.9759\n",
      "Epoch 483/600\n",
      " - 0s - loss: 0.0144 - acc: 0.9853 - val_loss: 0.0297 - val_acc: 0.9569\n",
      "Epoch 484/600\n",
      " - 0s - loss: 0.0180 - acc: 0.9784 - val_loss: 0.0164 - val_acc: 0.9848\n",
      "Epoch 485/600\n",
      " - 0s - loss: 0.0146 - acc: 0.9840 - val_loss: 0.0219 - val_acc: 0.9659\n",
      "Epoch 486/600\n",
      " - 0s - loss: 0.0149 - acc: 0.9848 - val_loss: 0.0159 - val_acc: 0.9824\n",
      "Epoch 487/600\n",
      " - 0s - loss: 0.0143 - acc: 0.9862 - val_loss: 0.0159 - val_acc: 0.9848\n",
      "Epoch 488/600\n",
      " - 0s - loss: 0.0141 - acc: 0.9861 - val_loss: 0.0303 - val_acc: 0.9644\n",
      "Epoch 489/600\n",
      " - 0s - loss: 0.0178 - acc: 0.9785 - val_loss: 0.0202 - val_acc: 0.9764\n",
      "Epoch 490/600\n",
      " - 0s - loss: 0.0163 - acc: 0.9815 - val_loss: 0.0174 - val_acc: 0.9796\n",
      "Epoch 491/600\n",
      " - 0s - loss: 0.0140 - acc: 0.9862 - val_loss: 0.0199 - val_acc: 0.9760\n",
      "Epoch 492/600\n",
      " - 0s - loss: 0.0144 - acc: 0.9853 - val_loss: 0.0179 - val_acc: 0.9781\n",
      "Epoch 493/600\n",
      " - 0s - loss: 0.0145 - acc: 0.9838 - val_loss: 0.0173 - val_acc: 0.9832\n",
      "Epoch 494/600\n",
      " - 0s - loss: 0.0152 - acc: 0.9830 - val_loss: 0.0159 - val_acc: 0.9818\n",
      "Epoch 495/600\n",
      " - 0s - loss: 0.0137 - acc: 0.9861 - val_loss: 0.0237 - val_acc: 0.9657\n",
      "Epoch 496/600\n",
      " - 0s - loss: 0.0172 - acc: 0.9818 - val_loss: 0.0174 - val_acc: 0.9811\n",
      "Epoch 497/600\n",
      " - 0s - loss: 0.0139 - acc: 0.9858 - val_loss: 0.0177 - val_acc: 0.9796\n",
      "Epoch 498/600\n",
      " - 0s - loss: 0.0148 - acc: 0.9838 - val_loss: 0.0165 - val_acc: 0.9798\n",
      "Epoch 499/600\n",
      " - 0s - loss: 0.0137 - acc: 0.9862 - val_loss: 0.0165 - val_acc: 0.9776\n",
      "Epoch 500/600\n",
      " - 0s - loss: 0.0135 - acc: 0.9870 - val_loss: 0.0193 - val_acc: 0.9827\n",
      "Epoch 501/600\n",
      " - 0s - loss: 0.0178 - acc: 0.9787 - val_loss: 0.0210 - val_acc: 0.9790\n",
      "Epoch 502/600\n",
      " - 0s - loss: 0.0151 - acc: 0.9840 - val_loss: 0.0166 - val_acc: 0.9781\n",
      "Epoch 503/600\n",
      " - 0s - loss: 0.0138 - acc: 0.9861 - val_loss: 0.0328 - val_acc: 0.9623\n",
      "Epoch 504/600\n",
      " - 0s - loss: 0.0169 - acc: 0.9813 - val_loss: 0.0164 - val_acc: 0.9794\n",
      "Epoch 505/600\n",
      " - 0s - loss: 0.0143 - acc: 0.9848 - val_loss: 0.0157 - val_acc: 0.9811\n",
      "Epoch 506/600\n",
      " - 0s - loss: 0.0144 - acc: 0.9847 - val_loss: 0.0154 - val_acc: 0.9832\n",
      "Epoch 507/600\n",
      " - 0s - loss: 0.0135 - acc: 0.9872 - val_loss: 0.0153 - val_acc: 0.9848\n",
      "Epoch 508/600\n",
      " - 0s - loss: 0.0136 - acc: 0.9867 - val_loss: 0.0155 - val_acc: 0.9848\n",
      "Epoch 509/600\n",
      " - 0s - loss: 0.0142 - acc: 0.9852 - val_loss: 0.0197 - val_acc: 0.9808\n",
      "Epoch 510/600\n",
      " - 0s - loss: 0.0146 - acc: 0.9851 - val_loss: 0.0165 - val_acc: 0.9848\n",
      "Epoch 511/600\n",
      " - 0s - loss: 0.0142 - acc: 0.9862 - val_loss: 0.0179 - val_acc: 0.9741\n",
      "Epoch 512/600\n",
      " - 0s - loss: 0.0155 - acc: 0.9826 - val_loss: 0.0226 - val_acc: 0.9713\n",
      "Epoch 513/600\n",
      " - 0s - loss: 0.0148 - acc: 0.9840 - val_loss: 0.0161 - val_acc: 0.9867\n",
      "Epoch 514/600\n",
      " - 0s - loss: 0.0138 - acc: 0.9860 - val_loss: 0.0173 - val_acc: 0.9781\n",
      "Epoch 515/600\n",
      " - 0s - loss: 0.0142 - acc: 0.9845 - val_loss: 0.0160 - val_acc: 0.9848\n",
      "Epoch 516/600\n",
      " - 0s - loss: 0.0135 - acc: 0.9874 - val_loss: 0.0159 - val_acc: 0.9829\n",
      "Epoch 517/600\n",
      " - 0s - loss: 0.0140 - acc: 0.9855 - val_loss: 0.0607 - val_acc: 0.9259\n",
      "Epoch 518/600\n",
      " - 0s - loss: 0.0256 - acc: 0.9693 - val_loss: 0.0227 - val_acc: 0.9674\n",
      "Epoch 519/600\n",
      " - 0s - loss: 0.0157 - acc: 0.9820 - val_loss: 0.0171 - val_acc: 0.9781\n",
      "Epoch 520/600\n",
      " - 0s - loss: 0.0138 - acc: 0.9858 - val_loss: 0.0157 - val_acc: 0.9848\n",
      "Epoch 521/600\n",
      " - 0s - loss: 0.0135 - acc: 0.9867 - val_loss: 0.0150 - val_acc: 0.9848\n",
      "Epoch 522/600\n",
      " - 0s - loss: 0.0141 - acc: 0.9857 - val_loss: 0.0270 - val_acc: 0.9560\n",
      "Epoch 523/600\n",
      " - 0s - loss: 0.0181 - acc: 0.9775 - val_loss: 0.0155 - val_acc: 0.9886\n",
      "Epoch 524/600\n",
      " - 0s - loss: 0.0143 - acc: 0.9853 - val_loss: 0.0155 - val_acc: 0.9824\n",
      "Epoch 525/600\n",
      " - 0s - loss: 0.0138 - acc: 0.9857 - val_loss: 0.0283 - val_acc: 0.9671\n",
      "Epoch 526/600\n",
      " - 0s - loss: 0.0163 - acc: 0.9817 - val_loss: 0.0175 - val_acc: 0.9781\n",
      "Epoch 527/600\n",
      " - 0s - loss: 0.0136 - acc: 0.9864 - val_loss: 0.0152 - val_acc: 0.9848\n",
      "Epoch 528/600\n",
      " - 0s - loss: 0.0133 - acc: 0.9871 - val_loss: 0.0152 - val_acc: 0.9832\n",
      "Epoch 529/600\n",
      " - 0s - loss: 0.0145 - acc: 0.9838 - val_loss: 0.0166 - val_acc: 0.9840\n",
      "Epoch 530/600\n",
      " - 0s - loss: 0.0135 - acc: 0.9867 - val_loss: 0.0155 - val_acc: 0.9867\n",
      "Epoch 531/600\n",
      " - 0s - loss: 0.0134 - acc: 0.9867 - val_loss: 0.0215 - val_acc: 0.9760\n",
      "Epoch 532/600\n",
      " - 0s - loss: 0.0153 - acc: 0.9830 - val_loss: 0.0167 - val_acc: 0.9866\n",
      "Epoch 533/600\n",
      " - 0s - loss: 0.0135 - acc: 0.9869 - val_loss: 0.0151 - val_acc: 0.9834\n",
      "Epoch 534/600\n",
      " - 0s - loss: 0.0135 - acc: 0.9864 - val_loss: 0.0315 - val_acc: 0.9623\n",
      "Epoch 535/600\n",
      " - 0s - loss: 0.0188 - acc: 0.9786 - val_loss: 0.0172 - val_acc: 0.9750\n",
      "Epoch 536/600\n",
      " - 0s - loss: 0.0134 - acc: 0.9863 - val_loss: 0.0261 - val_acc: 0.9633\n",
      "Epoch 537/600\n",
      " - 0s - loss: 0.0155 - acc: 0.9829 - val_loss: 0.0153 - val_acc: 0.9824\n",
      "Epoch 538/600\n",
      " - 0s - loss: 0.0135 - acc: 0.9859 - val_loss: 0.0176 - val_acc: 0.9796\n",
      "Epoch 539/600\n",
      " - 0s - loss: 0.0147 - acc: 0.9841 - val_loss: 0.0161 - val_acc: 0.9818\n",
      "Epoch 540/600\n",
      " - 0s - loss: 0.0152 - acc: 0.9825 - val_loss: 0.0193 - val_acc: 0.9753\n",
      "Epoch 541/600\n",
      " - 0s - loss: 0.0138 - acc: 0.9849 - val_loss: 0.0148 - val_acc: 0.9848\n",
      "Epoch 542/600\n",
      " - 0s - loss: 0.0135 - acc: 0.9857 - val_loss: 0.0170 - val_acc: 0.9814\n",
      "Epoch 543/600\n",
      " - 0s - loss: 0.0132 - acc: 0.9870 - val_loss: 0.0151 - val_acc: 0.9846\n",
      "Epoch 544/600\n",
      " - 0s - loss: 0.0132 - acc: 0.9863 - val_loss: 0.0233 - val_acc: 0.9674\n",
      "Epoch 545/600\n",
      " - 0s - loss: 0.0146 - acc: 0.9835 - val_loss: 0.0159 - val_acc: 0.9867\n",
      "Epoch 546/600\n",
      " - 0s - loss: 0.0133 - acc: 0.9871 - val_loss: 0.0255 - val_acc: 0.9639\n",
      "Epoch 547/600\n",
      " - 0s - loss: 0.0165 - acc: 0.9805 - val_loss: 0.0996 - val_acc: 0.9004\n",
      "Epoch 548/600\n",
      " - 0s - loss: 0.0360 - acc: 0.9580 - val_loss: 0.0172 - val_acc: 0.9777\n",
      "Epoch 549/600\n",
      " - 0s - loss: 0.0137 - acc: 0.9855 - val_loss: 0.0152 - val_acc: 0.9848\n",
      "Epoch 550/600\n",
      " - 0s - loss: 0.0133 - acc: 0.9869 - val_loss: 0.0168 - val_acc: 0.9796\n",
      "Epoch 551/600\n",
      " - 0s - loss: 0.0134 - acc: 0.9868 - val_loss: 0.0186 - val_acc: 0.9753\n",
      "Epoch 552/600\n",
      " - 0s - loss: 0.0144 - acc: 0.9843 - val_loss: 0.0166 - val_acc: 0.9833\n",
      "Epoch 553/600\n",
      " - 0s - loss: 0.0135 - acc: 0.9866 - val_loss: 0.0276 - val_acc: 0.9584\n",
      "Epoch 554/600\n",
      " - 0s - loss: 0.0188 - acc: 0.9785 - val_loss: 0.0249 - val_acc: 0.9700\n",
      "Epoch 555/600\n",
      " - 0s - loss: 0.0147 - acc: 0.9837 - val_loss: 0.0155 - val_acc: 0.9848\n",
      "Epoch 556/600\n",
      " - 0s - loss: 0.0138 - acc: 0.9860 - val_loss: 0.0238 - val_acc: 0.9674\n",
      "Epoch 557/600\n",
      " - 0s - loss: 0.0172 - acc: 0.9814 - val_loss: 0.0147 - val_acc: 0.9848\n",
      "Epoch 558/600\n",
      " - 0s - loss: 0.0131 - acc: 0.9871 - val_loss: 0.0147 - val_acc: 0.9848\n",
      "Epoch 559/600\n",
      " - 0s - loss: 0.0131 - acc: 0.9872 - val_loss: 0.0146 - val_acc: 0.9867\n",
      "Epoch 560/600\n",
      " - 0s - loss: 0.0129 - acc: 0.9876 - val_loss: 0.0148 - val_acc: 0.9848\n",
      "Epoch 561/600\n",
      " - 0s - loss: 0.0131 - acc: 0.9871 - val_loss: 0.0148 - val_acc: 0.9846\n",
      "Epoch 562/600\n",
      " - 0s - loss: 0.0131 - acc: 0.9875 - val_loss: 0.0170 - val_acc: 0.9804\n",
      "Epoch 563/600\n",
      " - 0s - loss: 0.0132 - acc: 0.9867 - val_loss: 0.0144 - val_acc: 0.9867\n",
      "Epoch 564/600\n",
      " - 0s - loss: 0.0132 - acc: 0.9871 - val_loss: 0.0146 - val_acc: 0.9848\n",
      "Epoch 565/600\n",
      " - 0s - loss: 0.0129 - acc: 0.9875 - val_loss: 0.0146 - val_acc: 0.9848\n",
      "Epoch 566/600\n",
      " - 0s - loss: 0.0127 - acc: 0.9878 - val_loss: 0.0167 - val_acc: 0.9781\n",
      "Epoch 567/600\n",
      " - 0s - loss: 0.0142 - acc: 0.9844 - val_loss: 0.0219 - val_acc: 0.9678\n",
      "Epoch 568/600\n",
      " - 0s - loss: 0.0156 - acc: 0.9837 - val_loss: 0.0201 - val_acc: 0.9733\n",
      "Epoch 569/600\n",
      " - 0s - loss: 0.0137 - acc: 0.9858 - val_loss: 0.0151 - val_acc: 0.9840\n",
      "Epoch 570/600\n",
      " - 0s - loss: 0.0132 - acc: 0.9869 - val_loss: 0.0177 - val_acc: 0.9760\n",
      "Epoch 571/600\n",
      " - 0s - loss: 0.0131 - acc: 0.9869 - val_loss: 0.0154 - val_acc: 0.9797\n",
      "Epoch 572/600\n",
      " - 0s - loss: 0.0127 - acc: 0.9875 - val_loss: 0.0197 - val_acc: 0.9734\n",
      "Epoch 573/600\n",
      " - 0s - loss: 0.0143 - acc: 0.9845 - val_loss: 0.0159 - val_acc: 0.9797\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 574/600\n",
      " - 0s - loss: 0.0140 - acc: 0.9849 - val_loss: 0.0164 - val_acc: 0.9781\n",
      "Epoch 575/600\n",
      " - 0s - loss: 0.0133 - acc: 0.9870 - val_loss: 0.0578 - val_acc: 0.9342\n",
      "Epoch 576/600\n",
      " - 0s - loss: 0.0260 - acc: 0.9709 - val_loss: 0.0183 - val_acc: 0.9762\n",
      "Epoch 577/600\n",
      " - 0s - loss: 0.0131 - acc: 0.9865 - val_loss: 0.0144 - val_acc: 0.9848\n",
      "Epoch 578/600\n",
      " - 0s - loss: 0.0129 - acc: 0.9875 - val_loss: 0.0174 - val_acc: 0.9741\n",
      "Epoch 579/600\n",
      " - 0s - loss: 0.0134 - acc: 0.9863 - val_loss: 0.0339 - val_acc: 0.9663\n",
      "Epoch 580/600\n",
      " - 0s - loss: 0.0174 - acc: 0.9817 - val_loss: 0.0248 - val_acc: 0.9674\n",
      "Epoch 581/600\n",
      " - 0s - loss: 0.0142 - acc: 0.9849 - val_loss: 0.0161 - val_acc: 0.9804\n",
      "Epoch 582/600\n",
      " - 0s - loss: 0.0127 - acc: 0.9872 - val_loss: 0.0151 - val_acc: 0.9852\n",
      "Epoch 583/600\n",
      " - 0s - loss: 0.0129 - acc: 0.9874 - val_loss: 0.0156 - val_acc: 0.9824\n",
      "Epoch 584/600\n",
      " - 0s - loss: 0.0131 - acc: 0.9863 - val_loss: 0.0193 - val_acc: 0.9752\n",
      "Epoch 585/600\n",
      " - 0s - loss: 0.0152 - acc: 0.9846 - val_loss: 0.0349 - val_acc: 0.9522\n",
      "Epoch 586/600\n",
      " - 0s - loss: 0.0180 - acc: 0.9796 - val_loss: 0.0153 - val_acc: 0.9848\n",
      "Epoch 587/600\n",
      " - 0s - loss: 0.0148 - acc: 0.9840 - val_loss: 0.0146 - val_acc: 0.9848\n",
      "Epoch 588/600\n",
      " - 0s - loss: 0.0130 - acc: 0.9873 - val_loss: 0.0302 - val_acc: 0.9650\n",
      "Epoch 589/600\n",
      " - 0s - loss: 0.0175 - acc: 0.9819 - val_loss: 0.0414 - val_acc: 0.9522\n",
      "Epoch 590/600\n",
      " - 0s - loss: 0.0326 - acc: 0.9589 - val_loss: 0.0294 - val_acc: 0.9684\n",
      "Epoch 591/600\n",
      " - 0s - loss: 0.0205 - acc: 0.9753 - val_loss: 0.0386 - val_acc: 0.9474\n",
      "Epoch 592/600\n",
      " - 0s - loss: 0.0194 - acc: 0.9745 - val_loss: 0.0208 - val_acc: 0.9743\n",
      "Epoch 593/600\n",
      " - 0s - loss: 0.0183 - acc: 0.9763 - val_loss: 0.0291 - val_acc: 0.9597\n",
      "Epoch 594/600\n",
      " - 0s - loss: 0.0187 - acc: 0.9755 - val_loss: 0.0207 - val_acc: 0.9696\n",
      "Epoch 595/600\n",
      " - 0s - loss: 0.0181 - acc: 0.9764 - val_loss: 0.0213 - val_acc: 0.9743\n",
      "Epoch 596/600\n",
      " - 0s - loss: 0.0173 - acc: 0.9784 - val_loss: 0.0187 - val_acc: 0.9746\n",
      "Epoch 597/600\n",
      " - 0s - loss: 0.0162 - acc: 0.9808 - val_loss: 0.0193 - val_acc: 0.9738\n",
      "Epoch 598/600\n",
      " - 0s - loss: 0.0179 - acc: 0.9780 - val_loss: 0.0210 - val_acc: 0.9780\n",
      "Epoch 599/600\n",
      " - 0s - loss: 0.0187 - acc: 0.9755 - val_loss: 0.0191 - val_acc: 0.9797\n",
      "Epoch 600/600\n",
      " - 0s - loss: 0.0161 - acc: 0.9809 - val_loss: 0.0175 - val_acc: 0.9778\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1db9ae93b38>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=600, batch_size=512,  verbose=2 ,validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 98.05%\n"
     ]
    }
   ],
   "source": [
    "# estimate accuracy on whole dataset using loaded weights\n",
    "scores = model.evaluate(X_train, y_train, verbose=0)\n",
    "print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
